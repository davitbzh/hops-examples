{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get started with Redshift and the Feature Store\n",
    "This tutorial notebook will help you get started working with the Hopsworks feature store and Redshift.\n",
    "\n",
    "## [Setup Redshift cluster](#setup_redshift)\n",
    "* [Create security group for Redshift cluster](#sg_redhsift)\n",
    "* [Create a sample Amazon Redshift cluster](#setup_redhsift)\n",
    "\n",
    "## [Access Redshift cluster](#access_redshift)\n",
    "###### To access Redshift cluster you can choose one of the alternatives bellow\n",
    "\n",
    "### [Alternative 1 - access with database username and password ](#user_pass)\n",
    "* [Create Redshift connector in Hopsworks](#jdbc_connector)\n",
    "\n",
    "### [Alternative 2 - access with IAM role](#iam_role)\n",
    "* [Create AIM role for EC2 instance to access Redshift cluster](#aim_ec2_redhsift)\n",
    "* [Attach AIM role to your hopsworks cluster](#attach_aim_ec2)\n",
    "\n",
    "## [Load telcom data into Redshift cluster](#load_data_redhsift)\n",
    "###### To load data into Redshift cluster you can choose one of the alternatives bellow\n",
    "### [Alternative 1 - From Hopsworks](#from_hopsworks)\n",
    "###### From hopsworks you can choose pandas or pyspark dataframes to load data\n",
    "* [Load using pandas](#load_data_redhsift_from_hopsworks_pd)\n",
    "* [Load using pyspark](#load_data_redhsift_from_hopswork_pysparks)\n",
    "\n",
    "### [Alternative 2 - From s3 bucket](#from_s3)\n",
    "* [Load sample data into Redshift cluster from s3](#load_data_redhsift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Redshift cluster  <a name=\"setup_redshift\"></a>\n",
    "\n",
    "## Create security group for Redshift cluster <a name=\"sg_redhsift\"></a>\n",
    "\n",
    "\n",
    "### From AWS management console go to VPC\n",
    "\n",
    "![1.jpg](images/VPC_steps/1.jpg)\n",
    "\n",
    "\n",
    "### Choose security groups\n",
    "![2.jpg](images/VPC_steps/2.jpg)\n",
    "\n",
    "\n",
    "### Create security groups\n",
    "\n",
    "![3.jpg](images/VPC_steps/3.jpg)\n",
    "\n",
    "\n",
    "### To add inbound rules for Redshift cluster traffic: choose Type as Redshift, Source as Custom  and enter sg name or port range of your hopsworks EC2 instance(s). \n",
    "\n",
    "![5.jpg](images/VPC_steps/5.jpg)\n",
    "\n",
    "\n",
    "## Create a sample Amazon Redshift cluster<a name=\"setup_redhsift\"></a>\n",
    "\n",
    "### Step 1) From AWS management console go to Redshift and select Create Cluster \n",
    "\n",
    "\n",
    "![1.jpg](images/redshift/1.png)\n",
    "\n",
    "\n",
    "### Step 2) From cluster configuration decide size of your Redshift cluster \n",
    "\n",
    "\n",
    "![2.jpg](images/redshift/2.png)\n",
    "\n",
    "\n",
    "### Step 3) Scroll down to Database Configuration and enter username and password\n",
    "\n",
    "![3.jpg](images/redshift/3.png)\n",
    "\n",
    "\n",
    "### Step 4) Scroll down to Cluster permissions. This is optional. However, If you want to load data from S3 add IAM service role that authorizes COPY, UNLOAD, and CREATE EXTERNAL SCHEMA operations\n",
    "\n",
    "\n",
    "![4.jpg](images/redshift/4.png)\n",
    "\n",
    "\n",
    "### Step 5) Scroll down Additional Configurations and add security group we created above \n",
    "\n",
    "![5.jpg](images/redshift/5.png)\n",
    "\n",
    "\n",
    "### Step 6) Scroll down and Create Cluster \n",
    "\n",
    "![6.jpg](images/redshift/6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access Redshift cluster  <a name=\"access_redshift\"></a> \n",
    "\n",
    "## Alternative 1 -  Access with database username and password <a name=\"user_pass\"></a> \n",
    "\n",
    "### Create Redshift connector in Hopsworks <a name=\"jdbc_connector\"></a>\n",
    "##### For this alternative you need to create Redshift connector from Hopsworks Feature Store. \n",
    "\n",
    "#### Step1 ) From Hopsworks go to Feature Store select Storage Connectors and click Create New\n",
    "![1.jpg](images/hopsworks/connector/1.png)\n",
    "\n",
    "#### Step2 ) Fill in name and description of the connector. <br>  \n",
    " &nbsp; In JDBC connection string tab paste following string but use your username, password, databasename and Redshift url: <br> \n",
    " &nbsp; redshift+driver_name://username:password@redshift_url:5439/databasename <br>\n",
    " &nbsp; <b>Note:</b> depending what type of driver you use need to replace <u>driver_name</u> accordingly. <br>\n",
    " &nbsp;&nbsp;&nbsp; <b>If you use psycopg2 library then your connection string will be:</b> <br> \n",
    " &nbsp;&nbsp;&nbsp; redshift+psycopg2://username:password@redshift_url:5439/databasename <br>\n",
    " &nbsp;&nbsp;&nbsp; <b>for redshift jdbc driver use following:</b> <br>\n",
    " &nbsp;&nbsp;&nbsp; jdbc:redshift://redshift_url:5439/databasename?user=username&password=password <br>\n",
    " &nbsp;&nbsp;&nbsp; <b>for redshift jdbc driver using IAM as authenticating method (follow instructions [Alternative 2 - access with IAM role](#iam_role)) use following:</b> <br>\n",
    " &nbsp;&nbsp;&nbsp; jdbc:redshift:iam://redshift_url:5439/databasename <br>\n",
    "                                                                                     \n",
    " &nbsp; then click Create\n",
    "![2.jpg](images/hopsworks/connector/2.png)\n",
    "\n",
    "\n",
    "## Alternative 2 - Access with IAM role <a name=\"iam_role\"></a>\n",
    "\n",
    "### Create IAM role for EC2 instance to access Redshift cluster <a name=\"aim_ec2_redhsift\"></a>\n",
    "\n",
    "#### Step 1 ) From AWS management console go to AIM Section\n",
    "\n",
    "![1.jpg](images/EC2_AIM/1.jpg)\n",
    "\n",
    "\n",
    "#### Step 2 ) Select roles\n",
    "\n",
    "![2.jpg](images/EC2_AIM/2.jpg)\n",
    "\n",
    "\n",
    "#### Step 3 ) Choose use case EC2\n",
    "\n",
    "![3.jpg](images/EC2_AIM/3.jpg)\n",
    "\n",
    "\n",
    "#### Step 4 ) Select necessary policy. For demo purposes we will select full access policy \n",
    "\n",
    "![4.jpg](images/EC2_AIM/4.jpg)\n",
    "\n",
    "\n",
    "#### Step 5 ) This step is optional. You may leave tags empty\n",
    "\n",
    "![5.jpg](images/EC2_AIM/5.jpg)\n",
    "\n",
    "\n",
    "#### Step6 ) In the final step create IAM role\n",
    "\n",
    "![6.jpg](images/EC2_AIM/6.jpg)\n",
    "\n",
    "### Attach IAM role to your Hopsworks cluster <a name=\"attach_aim_ec2\"></a>\n",
    "\n",
    "\n",
    "#### Step 1) From AWS management console go to EC2\n",
    "\n",
    "![1.jpg](images/EC2_change_AIM/1.jpg)\n",
    "\n",
    "\n",
    "#### Step 2) Select instances\n",
    "\n",
    "![2.jpg](images/EC2_change_AIM/2.jpg)\n",
    "\n",
    "\n",
    "#### Step 3) Go to Actions and select Instance Settings and then Attach/Replace IAM Role\n",
    "\n",
    "![3.jpg](images/EC2_change_AIM/3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load  sample data into Redshift cluster <a name=\"load_data_redhsift\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative 1 - Load telcom data into Redshift from hopsworks <a name=\"from_hopsworks\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load using pandas <a name=\"load_data_redhsift_from_hopsworks_pd\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine # tested on 0.8.1\n",
    "import psycopg2 # tested on 2.7.7\n",
    "\n",
    "from hops import featurestore\n",
    "\n",
    "connstr = featurestore.get_storage_connector(\"redshift_connector\",featurestore=featurestore.project_featurestore()).connection_string\n",
    "engine = create_engine(connstr)\n",
    "\n",
    "telcom = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"hdfs:///Projects/redshift/redshift_Training_Datasets/telco_customer_churn.csv\").toPandas()\n",
    "\n",
    "telcom.to_sql(\"telcom\", engine, index=False, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load using pyspark <a name=\"load_data_redhsift_from_hopswork_pysparks\"></a> \n",
    "\n",
    "### step 1) To use spark you need to provide Redshift JDBC driver. Download [Download an Amazon Redshift JDBC driver](https://docs.aws.amazon.com/redshift/latest/mgmt/configure-jdbc-connection.html#download-jdbc-driver)\n",
    "\n",
    "### step 2) upload jar file to hopsworks project\n",
    "![1.jpg](images/hopsworks/upload_jar/1.png)\n",
    "\n",
    "### step 3) Provide jar file when you launch spark cluster\n",
    "![2.jpg](images/hopsworks/upload_jar/2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import featurestore\n",
    "\n",
    "connstr = featurestore.get_storage_connector(\"redshift_connector\",featurestore=featurestore.project_featurestore()).connection_string\n",
    "\n",
    "telcom = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\")\\\n",
    ".csv(\"hdfs:///Projects/redshift/redshift_Training_Datasets/telco_customer_churn.csv\")\n",
    "\n",
    "telcom.\\\n",
    "write.\\\n",
    "format(\"jdbc\").\\\n",
    "option(\"driver\", \"com.amazon.redshift.jdbc42.Driver\").\\\n",
    "option(\"url\",connstr).\\\n",
    "option(\"dbtable\", \"telcom\").\\\n",
    "mode(\"overwrite\").\\\n",
    "save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Alternative 2 - Import a CSV in Redshift from s3 bucket <a name=\"from_s3\"></a>. \n",
    "Please follow tutorial [here](https://docs.aws.amazon.com/redshift/latest/dg/tutorial-loading-data.html) \n",
    "\n",
    "Importing a CSV into Redshift requires you to create a table first. \n",
    "\n",
    "<code>\n",
    "    CREATE TABLE telcom (\n",
    "        customer_id VARCHAR primary key \n",
    "        gender VARCHAR,   \n",
    "        senior_citizen VARCHAR, \n",
    "        partner VARCHAR,\n",
    "        dependents VARCHAR,  \n",
    "        tenure INTEGER, \n",
    "        phone_service VARCHAR,      \n",
    "        multiple_lines VARCHAR, \n",
    "        internet_service VARCHAR, \n",
    "        online_security VARCHAR, \n",
    "        online_backup VARCHAR, \n",
    "        device_protection VARCHAR, \n",
    "        tech_support VARCHAR, \n",
    "        streaming_tv VARCHAR,\n",
    "        streaming_movies VARCHAR,        \n",
    "        contract VARCHAR,\n",
    "        paperless_billing VARCHAR,            \n",
    "        payment_method INTEGER, \n",
    "        monthly_charges INTEGER, \n",
    "        total_charges INTEGER, \n",
    "        churn VARCHAR    \n",
    "    );\n",
    "</code>\n",
    "\n",
    "and then copy\n",
    "\n",
    "<code>\n",
    "    COPY telcom\n",
    "        FROM 's3://<your-bucket-name>/load/file_name.csv'\n",
    "        credentials 'aws_access_key_id=<Your-Access-Key-ID>;aws_secret_access_key=<Your-Secret-Access-Key>'\n",
    "    CSV;\n",
    "</code>\n",
    "    \n",
    "please refer to the [Redshift COPY Command Specification](https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html) for a complete list of options for COPY,     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
