{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time travel operations in Hopsworks Feature Store\n",
    "\n",
    "In this notebook we will introduce time travel operations in Hopsworks Feature Store (HSFS). Currently HSFS supports Apache Hudi (http://hudi.apache.org/) a storage abstraction/library for doing **incremental** data ingestion to a Hopsworks Feature Store.\n",
    "\n",
    "TLDR; Hudi is a storage abstraction/library build on top of Spark. A Hudi dataset stores data in Parquet files and maintains additional metadata to make upserts efficient. A Hudi ingest job is intended to be run as a streaming ingest job, on an interval such as every 15 minutes, reading deltas from a message-bus like Kafka and ingesting the deltas **incrementally** into a data lake.\n",
    "\n",
    "![Incremental ETL](./../images/incr_load.png \"Incremetal ETL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "Hudi is an open-source library for doing incremental ingestion of data for large analytical datasets stored on distributed file systems. The library was originally developed at Uber to improve their data latency, but  it is now an Apache project.\n",
    "\n",
    "The main motivation for Hudi is that it reduces the **data latency** for ingesting large datasets into data lakes. Traditional ETL typically involves taking a snapshot of a production database and doing a full load into a data lake (typically stored on a distributed file system). Using the snapshot approach for ETL is simple since the snapshot is immutable and can be loaded as an atomic unit into the data lake. However, the con of taking this approach to doing data ingestion is that it is *slow*. Even if just a single record have been updated since the last data ingestion, the entire table has to be re-written. If you are working with Big Data (TB or PB size datasets) then this will introduce significant *data latency* (up to 24 hours in Uber's case) and *wasted resources* (majority of the writes when ingesting the snapshot is redundant as most of the records have not been updated since the last ETL step). \n",
    "\n",
    "This motivates the use-case for **incremental** data ingestion. Incremental data ingestion means that only deltas/changelogs since the last ingestion are inserted. \n",
    "\n",
    "Incremental ingestion lies in-between traditional batch ingestion and the streaming use-case. It can provide data latency as low as *minutes* for petabyte-scale datasets. The incremental mode for processing introduces new trade-offs compared to streaming and batch. It has lower data latency than traditional batch processing, but a slightly higher latency than stream processing. Why not go full-streaming instead of the incremental processing? It boils down to your requirements and trade-offs. If you need data latency in the order of seconds, then you have to use stream processing (e.g fraud detection). However if your business can do with data latency in the order of say 5 minutes (applications which are fine with this latency could be feature engineering pipelines, building dashboards, or doing near-real-time analytics), then incremental processing really shines. \n",
    "\n",
    "With incremental processing, you process data in *mini-batches* and run the spark job frequently, every 15 minutes or so. By using mini-batches rather than record-by-record streaming, the incremental model makes better use of resources and makes it easier to do complex processing and joins which are more suited for the batch-style of processing rather than stream-processing.\n",
    "\n",
    "![Near Real Time](./../images/near_real_time.jpg \"Near Real Time\")\n",
    "\n",
    "If the data is immutable by design, incremental processing can be done without any additional ingestion library, just use the *append* primitive supported in HDFS through some HDFS client, such as Spark, e.g:\n",
    "\n",
    "```scala\n",
    "newRecordsDf = (...)\n",
    "newRecordsDf.write.format(\"hive\").mode(\"append\").insertInto(tableName)\n",
    "```\n",
    "\n",
    "Unfortunately, data is rarely immutable in practice. A bank transaction might be reverted, a customer might change his or her home adress, and a customer review might be updated, to give a few examples. This is where Hudi comes into the picture. Hudi stands for `Hadoop Upserts anD Incrementals` and brings two new primitives for data engineering on distributed file systems (in addition to append/read):\n",
    "\n",
    "- `Upsert`: the ability to do insertions (appends) and updates efficiently. \n",
    "- `Incremental reads`: the ability to read datasets incrementally using the notion of \"commits\".\n",
    "\n",
    "![Upserts](./../images/upsert_illustration.png \"Upserts\")\n",
    "\n",
    "Lets consider the process of updating a single record in a data lake of Parquet files stored on a distributed file system. Without using Hudi, this would entail scanning the entire dataset to find the record in order to do the update and then rewrite the entire dataset: \n",
    "\n",
    "```scala\n",
    "updatedRecordsDf = (...)\n",
    "updatedRecordsDf.write.format(\"hive\").mode(\"overwrite\").insertInto(tableName) \n",
    "```\n",
    "\n",
    "This does not scale and HDFS/Parquet is not designed for this use-case. With Hudi, the upsert operation is a first-class primitive in the ingestion framework and it is optimized to be fast using index-lookups and atomic updates. We will see how we can use Hudi for this purpose later on in the notebook, but essentially it is as simple as :\n",
    "\n",
    "```scala\n",
    "updatedRecordsDf = (...)\n",
    "upsertDf.write.format(\"org.apache.hudi\")\n",
    "              .option(\"hoodie.datasource.write.operation\", \"upsert\")\n",
    "              ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Hudi\n",
    "\n",
    "Hudi is a Spark library that is intended to be run as a streaming ingest job, and ingests data as mini-batches (typically on the order of one to two minutes). A Hudi job generally reads delta-updates from a message-bus like Kafka, and upserts them into a data lake stored on a distributed file system. By maintaining bloom indexes and commit logs, Hudi provide ACID transactions, time-travel and scalable upserts.\n",
    "\n",
    "![Hudi Dataset](./../images/hudi_dataset.png \"Hudi Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Hopsworks Feature Store time travel operations can be used for ML and Feature Pipelines\n",
    "\n",
    "Hudi is integrated in the Hopsworks Feature Store for doing incremental feature computation and for point-in-time correctness and backfilling of feature data.\n",
    "\n",
    "![Incremental Feature Engineering](./../images/featurestore_incremental_pull.png \"Incremetal Feature Engineering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>22</td><td>application_1604621247318_0036</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hopsworks0.logicalclocks.com:8088/proxy/application_1604621247318_0036/\">Link</a></td><td><a target=\"_blank\" href=\"http://hopsworks0.logicalclocks.com:8042/node/containerlogs/container_e01_1604621247318_0036_01_000001/demo_fs_meb10000__meb10000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "res1: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@26daa88\n"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create HUDI time travel enabled feature group and Bulk Insert Sample Dataset\n",
    "\n",
    "For this demo we will use small sample of the Agarwal Generator that is a widely used dataset. It contains the hypothetical data of people applying for a loan. `Rakesh Agrawal, Tomasz Imielinksi, and Arun Swami, \"Database Mining: A Performance Perspective\", IEEE Transactions on Knowledge and Data Engineering, 5(6), December 1993. <br/><br/>`\n",
    "\n",
    "##### For simplicity of demo purposes we will split Agarwal dataset into 3 freature groups and manualy create datasets: \n",
    "* `economy_fg` with customer id, salary, loan, value of house, age of house, commission and type of car features; \n",
    "* `demographic_fg` with customer id, age, education level, zip code,\n",
    "* `class_fg` which will contain labels wether loan was approved `class B` or rejected `class A`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import com.logicalclocks.hsfs._\n",
      "import scala.collection.JavaConversions._\n",
      "import collection.JavaConverters._\n",
      "import org.apache.spark.sql.{DataFrame, Row}\n",
      "import org.apache.spark.sql.catalyst.expressions.GenericRow\n",
      "import org.apache.spark.sql.types._\n",
      "import java.sql.Date\n",
      "import java.sql.Timestamp\n",
      "connection: com.logicalclocks.hsfs.HopsworksConnection = com.logicalclocks.hsfs.HopsworksConnection@1af1cc43\n",
      "fs: com.logicalclocks.hsfs.FeatureStore = FeatureStore{id=67, name='demo_fs_meb10000_featurestore', projectId=119, featureGroupApi=com.logicalclocks.hsfs.metadata.FeatureGroupApi@68518159}\n"
     ]
    }
   ],
   "source": [
    "import com.logicalclocks.hsfs._\n",
    "import scala.collection.JavaConversions._\n",
    "import collection.JavaConverters._\n",
    "\n",
    "import org.apache.spark.sql.{ DataFrame, Row }\n",
    "import org.apache.spark.sql.catalyst.expressions.GenericRow\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "import java.sql.Date\n",
    "import java.sql.Timestamp\n",
    "\n",
    "val connection = HopsworksConnection.builder().build();\n",
    "val fs = connection.getFeatureStore();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "economyFgSchema: List[org.apache.spark.sql.types.StructField] = List(StructField(id,IntegerType,true), StructField(salary,FloatType,true), StructField(commission,FloatType,true), StructField(car,StringType,true), StructField(hvalue,FloatType,true), StructField(hyears,IntegerType,true), StructField(loan,FloatType,true), StructField(year,IntegerType,true))\n",
      "demographicFgSchema: List[org.apache.spark.sql.types.StructField] = List(StructField(id,IntegerType,true), StructField(age,IntegerType,true), StructField(elevel,StringType,true), StructField(zipcode,StringType,true))\n",
      "classFgSchema: List[org.apache.spark.sql.types.StructField] = List(StructField(id,IntegerType,true), StructField(class,StringType,true), StructField(year,IntegerType,true))\n"
     ]
    }
   ],
   "source": [
    "val economyFgSchema = \n",
    " scala.collection.immutable.List(\n",
    "  StructField(\"id\", IntegerType, true),\n",
    "  StructField(\"salary\", FloatType, true),\n",
    "  StructField(\"commission\", FloatType, true),\n",
    "  StructField(\"car\", StringType, true), \n",
    "  StructField(\"hvalue\", FloatType, true),      \n",
    "  StructField(\"hyears\", IntegerType, true),     \n",
    "  StructField(\"loan\", FloatType, true),\n",
    "  StructField(\"year\", IntegerType, true)          \n",
    ")\n",
    "\n",
    "val demographicFgSchema = \n",
    " scala.collection.immutable.List(\n",
    "  StructField(\"id\", IntegerType, true),\n",
    "  StructField(\"age\", IntegerType, true),\n",
    "  StructField(\"elevel\", StringType, true),   \n",
    "  StructField(\"zipcode\", StringType, true) \n",
    ")\n",
    "\n",
    "val classFgSchema = \n",
    " scala.collection.immutable.List(\n",
    "  StructField(\"id\", IntegerType, true),\n",
    "  StructField(\"class\", StringType, true),\n",
    "  StructField(\"year\", IntegerType, true)          \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create spark dataframes for each Feature groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "economyBulkInsertData: Seq[org.apache.spark.sql.Row] = List([1,110499.73,0.0,car15,235000.0,30,354724.2,2020], [2,140893.77,0.0,car20,135000.0,2,395015.34,2020], [3,119159.65,0.0,car1,145000.0,22,122025.08,2020], [4,20000.0,52593.63,car9,185000.0,30,99629.62,2020])\n",
      "economyBulkInsertDf: org.apache.spark.sql.DataFrame = [id: int, salary: float ... 6 more fields]\n"
     ]
    }
   ],
   "source": [
    "val economyBulkInsertData = Seq(\n",
    "    Row(1, 110499.73f, 0.0f,  \"car15\",  235000.0f, 30, 354724.18f, 2020),\n",
    "    Row(2, 140893.77f, 0.0f,  \"car20\",  135000.0f, 2, 395015.33f, 2020),\n",
    "    Row(3, 119159.65f, 0.0f,  \"car1\", 145000.0f, 22, 122025.08f, 2020),\n",
    "    Row(4, 20000.0f, 52593.63f, \"car9\", 185000.0f, 30, 99629.62f, 2020)\n",
    ")\n",
    "\n",
    "val economyBulkInsertDf = spark.createDataFrame(\n",
    "    spark.sparkContext.parallelize(economyBulkInsertData),\n",
    "    StructType(economyFgSchema)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+-----+--------+------+---------+----+\n",
      "| id|   salary|commission|  car|  hvalue|hyears|     loan|year|\n",
      "+---+---------+----------+-----+--------+------+---------+----+\n",
      "|  1|110499.73|       0.0|car15|235000.0|    30| 354724.2|2020|\n",
      "|  2|140893.77|       0.0|car20|135000.0|     2|395015.34|2020|\n",
      "|  3|119159.65|       0.0| car1|145000.0|    22|122025.08|2020|\n",
      "|  4|  20000.0|  52593.63| car9|185000.0|    30| 99629.62|2020|\n",
      "+---+---------+----------+-----+--------+------+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "economyBulkInsertDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demographicBulkInsertData: Seq[org.apache.spark.sql.Row] = List([1,54,level3,zipcode5], [2,44,level4,zipcode8], [3,49,level2,zipcode4], [4,56,level0,zipcode2])\n",
      "demographicBulkInsertDf: org.apache.spark.sql.DataFrame = [id: int, age: int ... 2 more fields]\n"
     ]
    }
   ],
   "source": [
    "val demographicBulkInsertData = Seq(\n",
    "    Row(1, 54, \"level3\", \"zipcode5\"),\n",
    "    Row(2, 44, \"level4\", \"zipcode8\"),\n",
    "    Row(3, 49, \"level2\", \"zipcode4\"),\n",
    "    Row(4, 56, \"level0\", \"zipcode2\")\n",
    ")\n",
    "\n",
    "val demographicBulkInsertDf = spark.createDataFrame(\n",
    "    spark.sparkContext.parallelize(demographicBulkInsertData),\n",
    "    StructType(demographicFgSchema)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+--------+\n",
      "| id|age|elevel| zipcode|\n",
      "+---+---+------+--------+\n",
      "|  1| 54|level3|zipcode5|\n",
      "|  2| 44|level4|zipcode8|\n",
      "|  3| 49|level2|zipcode4|\n",
      "|  4| 56|level0|zipcode2|\n",
      "+---+---+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographicBulkInsertDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classBulkInsertData: Seq[org.apache.spark.sql.Row] = List([1,groupB,2020], [2,groupB,2020], [3,groupB,2020], [4,groupB,2020])\n",
      "classBulkInsertDf: org.apache.spark.sql.DataFrame = [id: int, class: string ... 1 more field]\n"
     ]
    }
   ],
   "source": [
    "val classBulkInsertData = Seq(\n",
    "    Row(1, \"groupB\", 2020),\n",
    "    Row(2, \"groupB\", 2020),\n",
    "    Row(3, \"groupB\", 2020),\n",
    "    Row(4, \"groupB\", 2020)\n",
    ") \n",
    "\n",
    "val classBulkInsertDf = spark.createDataFrame(\n",
    "    spark.sparkContext.parallelize(classBulkInsertData),\n",
    "    StructType(classFgSchema)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+\n",
      "| id| class|year|\n",
      "+---+------+----+\n",
      "|  1|groupB|2020|\n",
      "|  2|groupB|2020|\n",
      "|  3|groupB|2020|\n",
      "|  4|groupB|2020|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classBulkInsertDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create feature groups \n",
    "\n",
    "Now we will create each feature group and enable time travel format `TimeTravelFormat.HUDI`. In Hopsworks Feature Store `primary` and `partition` keys are required to be privided for HUDI enabled feature groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "economyFg: com.logicalclocks.hsfs.FeatureGroup = com.logicalclocks.hsfs.FeatureGroup@5daf72d7\n"
     ]
    }
   ],
   "source": [
    "val economyFg = (fs.createFeatureGroup().\n",
    "                name(\"economy_fg\").\n",
    "                description(\"Hudi Household Economy Feature Group\").\n",
    "                version(1).\n",
    "                primaryKeys(Seq(\"id\")).\n",
    "                partitionKeys(Seq(\"year\")).\n",
    "                timeTravelFormat(TimeTravelFormat.HUDI).\n",
    "                build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demographyFg: com.logicalclocks.hsfs.FeatureGroup = com.logicalclocks.hsfs.FeatureGroup@76fc27b5\n"
     ]
    }
   ],
   "source": [
    "val demographyFg = (fs.createFeatureGroup().\n",
    "                    name(\"demography_fg\").\n",
    "                    description(\"Hudi Demographic Feature Group\").\n",
    "                    version(1).\n",
    "                    primaryKeys(Seq(\"id\")).\n",
    "                    partitionKeys(Seq(\"zipcode\")).\n",
    "                    timeTravelFormat(TimeTravelFormat.HUDI).\n",
    "                    build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classFg: com.logicalclocks.hsfs.FeatureGroup = com.logicalclocks.hsfs.FeatureGroup@680629ee\n"
     ]
    }
   ],
   "source": [
    "val classFg = (fs.createFeatureGroup().\n",
    "                name(\"class_fg\").\n",
    "                description(\"Hudi Class Feature Group\").\n",
    "                version(1).\n",
    "                primaryKeys(Seq(\"id\")).\n",
    "                partitionKeys(Seq(\"year\")).\n",
    "                timeTravelFormat(TimeTravelFormat.HUDI).\n",
    "                build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulk insert data into the feature group\n",
    "Since we have not yet saved any data into newly created feature groups we will use Apache hudi terminology and `Bulk Insert` data. In HSFS its just issuing `save` method.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "economyFg.save(economyBulkInsertDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographyFg.save(demographicBulkInsertDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "classFg.save(classBulkInsertDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hopsworks Feature Store Commits\n",
    "\n",
    "If you thoroughly followed this demo you probably noticed that Hopsworks Feature Store uses Apache Hudi as its time travel engine. Hudi introduces the notion of `commits` which means that it supports certain properties of traditional databases such as single-table transactions, snapshot isolation, atomic upserts and savepoints for data recovery. If an ingestion fails for some reason, no partial results will be written rather the ingestion will be roll-backed. The commit is implemented using atomic `mv` operation in HDFS. \n",
    "\n",
    "Currently, feature groups that we created contain only a single commit each as we've just done a single bulk-insert. Lets explore time line of `economyFg`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604795225000,{committedOn=20201108002705, rowsUpdated=0, rowsDeleted=0, rowsInserted=4})\n"
     ]
    }
   ],
   "source": [
    "for ((k,v) <- economyFg.commitDetails()){\n",
    "    println (k,v)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+-----+--------+------+---------+----+\n",
      "| id|   salary|commission|  car|  hvalue|hyears|     loan|year|\n",
      "+---+---------+----------+-----+--------+------+---------+----+\n",
      "|  2|140893.77|       0.0|car20|135000.0|     2|395015.34|2020|\n",
      "|  1|110499.73|       0.0|car15|235000.0|    30| 354724.2|2020|\n",
      "|  4|  20000.0|  52593.63| car9|185000.0|    30| 99629.62|2020|\n",
      "|  3|119159.65|       0.0| car1|145000.0|    22|122025.08|2020|\n",
      "+---+---------+----------+-----+--------+------+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "economyFg.read().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+--------+\n",
      "| id|age|elevel| zipcode|\n",
      "+---+---+------+--------+\n",
      "|  3| 49|level2|zipcode4|\n",
      "|  2| 44|level4|zipcode8|\n",
      "|  4| 56|level0|zipcode2|\n",
      "|  1| 54|level3|zipcode5|\n",
      "+---+---+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographyFg.read().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+\n",
      "| id| class|year|\n",
      "+---+------+----+\n",
      "|  2|groupB|2020|\n",
      "|  3|groupB|2020|\n",
      "|  4|groupB|2020|\n",
      "|  1|groupB|2020|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classFg.read().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsert new data into a Feature Group\n",
    "\n",
    "So far we have not done anything time travel special, we simply did a regular bulk-insert of some data into a Hudi enabled feature group. We could have done the same thing using just regular None Hudi enabled Feature group. However now we will look into how we can do upserts, and how Hopsworks Feature store enables us to do this efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Sample Upserts Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "economyUpsertData: Seq[org.apache.spark.sql.Row] = List([1,120499.73,0.0,car17,205000.0,30,564724.2,2020], [2,160893.77,0.0,car10,179000.0,2,455015.34,2020], [5,93956.32,0.0,car15,135000.0,1,458679.8,2020], [6,41365.43,52809.15,car7,135000.0,19,216839.7,2020], [7,94805.61,0.0,car17,135000.0,23,233216.06,2020])\n",
      "economyUpsertDf: org.apache.spark.sql.DataFrame = [id: int, salary: float ... 6 more fields]\n",
      "+---+---------+----------+-----+--------+------+---------+----+\n",
      "| id|   salary|commission|  car|  hvalue|hyears|     loan|year|\n",
      "+---+---------+----------+-----+--------+------+---------+----+\n",
      "|  1|120499.73|       0.0|car17|205000.0|    30| 564724.2|2020|\n",
      "|  2|160893.77|       0.0|car10|179000.0|     2|455015.34|2020|\n",
      "|  5| 93956.32|       0.0|car15|135000.0|     1| 458679.8|2020|\n",
      "|  6| 41365.43|  52809.15| car7|135000.0|    19| 216839.7|2020|\n",
      "|  7| 94805.61|       0.0|car17|135000.0|    23|233216.06|2020|\n",
      "+---+---------+----------+-----+--------+------+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val economyUpsertData = Seq(\n",
    "    Row(1, 120499.73f, 0.0f, \"car17\", 205000.0f, 30, 564724.18f, 2020),    //update\n",
    "    Row(2, 160893.77f, 0.0f, \"car10\", 179000.0f, 2, 455015.33f, 2020),     //update\n",
    "    Row(5, 93956.32f, 0.0f, \"car15\",  135000.0f, 1, 458679.82f, 2020),     //insert\n",
    "    Row(6, 41365.43f, 52809.15f, \"car7\", 135000.0f, 19, 216839.71f, 2020), //insert\n",
    "    Row(7, 94805.61f, 0.0f, \"car17\", 135000.0f, 23, 233216.07f, 2020)      //insert\n",
    ")\n",
    "\n",
    "val economyUpsertDf = spark.createDataFrame(\n",
    "  spark.sparkContext.parallelize(economyUpsertData),\n",
    "  StructType(economyFgSchema)\n",
    ")\n",
    "\n",
    "economyUpsertDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demographicUpsertData: Seq[org.apache.spark.sql.Row] = List([2,44,level1,zipcode8], [5,59,level1,zipcode2], [6,71,level2,zipcode3], [7,32,level1,zipcode2])\n",
      "demographicUpsertDf: org.apache.spark.sql.DataFrame = [id: int, age: int ... 2 more fields]\n",
      "+---+---+------+--------+\n",
      "| id|age|elevel| zipcode|\n",
      "+---+---+------+--------+\n",
      "|  2| 44|level1|zipcode8|\n",
      "|  5| 59|level1|zipcode2|\n",
      "|  6| 71|level2|zipcode3|\n",
      "|  7| 32|level1|zipcode2|\n",
      "+---+---+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val demographicUpsertData = Seq(\n",
    "    Row(2, 44, \"level1\", \"zipcode8\"),     //update\n",
    "    Row(5, 59, \"level1\", \"zipcode2\"),     //insert\n",
    "    Row(6, 71, \"level2\", \"zipcode3\"),     //insert\n",
    "    Row(7, 32, \"level1\", \"zipcode2\")      //insert\n",
    ")\n",
    "\n",
    "val demographicUpsertDf = spark.createDataFrame(\n",
    "    spark.sparkContext.parallelize(demographicUpsertData),\n",
    "    StructType(demographicFgSchema)\n",
    ")\n",
    "\n",
    "demographicUpsertDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classUpsertData: Seq[org.apache.spark.sql.Row] = List([1,groupA,2020], [5,groupA,2020], [6,groupA,2020], [7,groupA,2020])\n",
      "classUpsertDf: org.apache.spark.sql.DataFrame = [id: int, class: string ... 1 more field]\n",
      "+---+------+----+\n",
      "| id| class|year|\n",
      "+---+------+----+\n",
      "|  1|groupA|2020|\n",
      "|  5|groupA|2020|\n",
      "|  6|groupA|2020|\n",
      "|  7|groupA|2020|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val classUpsertData = Seq(\n",
    "    Row(1, \"groupA\", 2020), //update\n",
    "    Row(5, \"groupA\", 2020), //insert\n",
    "    Row(6, \"groupA\", 2020), //insert\n",
    "    Row(7, \"groupA\", 2020)  //insert\n",
    ") \n",
    "\n",
    "val classUpsertDf = spark.createDataFrame(\n",
    "    spark.sparkContext.parallelize(classUpsertData),\n",
    "    StructType(classFgSchema)\n",
    ")\n",
    "\n",
    "classUpsertDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make the Upsert using Hopsworks Feature Store API\n",
    "In Hopsworks Feature Store issuing `insert` method on Apache Hudi enabled feature group will by default perform `UPSERT` operation which means to either insert a new row, or on the basis of `parimary` and `partition` keys or update already existing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "economyFg.insert(economyUpsertDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographyFg.insert(demographicUpsertDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "classFg.insert(classUpsertDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the results\n",
    "\n",
    "Notice that although Hudi enabled Feature group stores the old value of the records from the previous commit, when you query it will only return the values of the latest commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+-----+--------+------+---------+----+\n",
      "| id|   salary|commission|  car|  hvalue|hyears|     loan|year|\n",
      "+---+---------+----------+-----+--------+------+---------+----+\n",
      "|  1|120499.73|       0.0|car17|205000.0|    30| 564724.2|2020|\n",
      "|  5| 93956.32|       0.0|car15|135000.0|     1| 458679.8|2020|\n",
      "|  6| 41365.43|  52809.15| car7|135000.0|    19| 216839.7|2020|\n",
      "|  7| 94805.61|       0.0|car17|135000.0|    23|233216.06|2020|\n",
      "|  2|160893.77|       0.0|car10|179000.0|     2|455015.34|2020|\n",
      "|  4|  20000.0|  52593.63| car9|185000.0|    30| 99629.62|2020|\n",
      "|  3|119159.65|       0.0| car1|145000.0|    22|122025.08|2020|\n",
      "+---+---------+----------+-----+--------+------+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "economyFg.read().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+--------+\n",
      "| id|age|elevel| zipcode|\n",
      "+---+---+------+--------+\n",
      "|  2| 44|level1|zipcode8|\n",
      "|  6| 71|level2|zipcode3|\n",
      "|  3| 49|level2|zipcode4|\n",
      "|  1| 54|level3|zipcode5|\n",
      "|  4| 56|level0|zipcode2|\n",
      "|  5| 59|level1|zipcode2|\n",
      "|  7| 32|level1|zipcode2|\n",
      "+---+---+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographyFg.read().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+\n",
      "| id| class|year|\n",
      "+---+------+----+\n",
      "|  1|groupA|2020|\n",
      "|  5|groupA|2020|\n",
      "|  6|groupA|2020|\n",
      "|  7|groupA|2020|\n",
      "|  2|groupB|2020|\n",
      "|  3|groupB|2020|\n",
      "|  4|groupB|2020|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classFg.read.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the updated commit timeline of `economyFg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604795370000,{committedOn=20201108002930, rowsUpdated=2, rowsDeleted=0, rowsInserted=3})\n",
      "(1604795225000,{committedOn=20201108002705, rowsUpdated=0, rowsDeleted=0, rowsInserted=4})\n"
     ]
    }
   ],
   "source": [
    "for ((k,v) <- economyFg.commitDetails()){\n",
    "    println (k,v)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ((k,v) <- demographyFg.commitDetails()){\n",
    "    println (k,v)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604795324000,{committedOn=20201108002844, rowsUpdated=0, rowsDeleted=0, rowsInserted=4})\n"
     ]
    }
   ],
   "source": [
    "for ((k,v) <- classFg.commitDetails()){\n",
    "    println (k,v)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets make one more commit to better demostrate time travel capabilities of Hopsworks Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "economyUpsertData: Seq[org.apache.spark.sql.Row] = List([8,64410.62,39884.39,car20,125000.0,6,350707.38,2020], [9,128298.82,0.0,car19,135000.0,12,20768.06,2020], [10,100806.92,0.0,car8,135000.0,6,293106.66,2020])\n",
      "economyUpsertDf: org.apache.spark.sql.DataFrame = [id: int, salary: float ... 6 more fields]\n",
      "+---+---------+----------+-----+--------+------+---------+----+\n",
      "| id|   salary|commission|  car|  hvalue|hyears|     loan|year|\n",
      "+---+---------+----------+-----+--------+------+---------+----+\n",
      "|  8| 64410.62|  39884.39|car20|125000.0|     6|350707.38|2020|\n",
      "|  9|128298.82|       0.0|car19|135000.0|    12| 20768.06|2020|\n",
      "| 10|100806.92|       0.0| car8|135000.0|     6|293106.66|2020|\n",
      "+---+---------+----------+-----+--------+------+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val economyUpsertData = Seq(    \n",
    "    Row(8, 64410.62f, 39884.39f, \"car20\",  125000.0f, 6, 350707.38f, 2020), //insert\n",
    "    Row(9, 128298.82f, 0.0f, \"car19\",  135000.0f, 12, 20768.06f, 2020),     //insert\n",
    "    Row(10,100806.92f, 0.0f, \"car8\", 135000.0f, 6, 293106.65f, 2020)        //insert   \n",
    "    \n",
    ")\n",
    "\n",
    "val economyUpsertDf = spark.createDataFrame(\n",
    "  spark.sparkContext.parallelize(economyUpsertData),\n",
    "  StructType(economyFgSchema)\n",
    ")\n",
    "\n",
    "economyUpsertDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demographicUpsertData: Seq[org.apache.spark.sql.Row] = List([8,33,level2,zipcode1], [9,32,level1,zipcode3], [10,58,level2,zipcode5])\n",
      "demographicUpsertDf: org.apache.spark.sql.DataFrame = [id: int, age: int ... 2 more fields]\n",
      "+---+---+------+--------+\n",
      "| id|age|elevel| zipcode|\n",
      "+---+---+------+--------+\n",
      "|  8| 33|level2|zipcode1|\n",
      "|  9| 32|level1|zipcode3|\n",
      "| 10| 58|level2|zipcode5|\n",
      "+---+---+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val demographicUpsertData = Seq(    \n",
    "    Row(8, 33, \"level2\", \"zipcode1\"),     //insert\n",
    "    Row(9, 32, \"level1\", \"zipcode3\"),     //insert\n",
    "    Row(10, 58, \"level2\", \"zipcode5\")     //insert    \n",
    ")\n",
    "\n",
    "val demographicUpsertDf = spark.createDataFrame(\n",
    "    spark.sparkContext.parallelize(demographicUpsertData),\n",
    "    StructType(demographicFgSchema)\n",
    ")\n",
    "\n",
    "demographicUpsertDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classUpsertData: Seq[org.apache.spark.sql.Row] = List([8,groupA,2020], [9,groupA,2020], [10,groupB,2020])\n",
      "classUpsertDf: org.apache.spark.sql.DataFrame = [id: int, class: string ... 1 more field]\n",
      "+---+------+----+\n",
      "| id| class|year|\n",
      "+---+------+----+\n",
      "|  8|groupA|2020|\n",
      "|  9|groupA|2020|\n",
      "| 10|groupB|2020|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val classUpsertData = Seq(\n",
    "    Row(8, \"groupA\", 2020), //insert\n",
    "    Row(9, \"groupA\", 2020), //insert\n",
    "    Row(10, \"groupB\", 2020) //insert    \n",
    ") \n",
    "\n",
    "val classUpsertDf = spark.createDataFrame(\n",
    "    spark.sparkContext.parallelize(classUpsertData),\n",
    "    StructType(classFgSchema)\n",
    ")\n",
    "\n",
    "classUpsertDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "economyFg.insert(economyUpsertDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographyFg.insert(demographicUpsertDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classFg.insert(classUpsertDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Travel Queries\n",
    "When `read` method is issued on `FeatureGroup` object, whithout any aparameters, most recent view of the Feature group will be returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+----------+-----+---------+--------+----+\n",
      "|     loan| id|hyears|commission|  car|   salary|  hvalue|year|\n",
      "+---------+---+------+----------+-----+---------+--------+----+\n",
      "| 564724.2|  1|    30|       0.0|car17|120499.73|205000.0|2020|\n",
      "| 458679.8|  5|     1|       0.0|car15| 93956.32|135000.0|2020|\n",
      "| 216839.7|  6|    19|  52809.15| car7| 41365.43|135000.0|2020|\n",
      "|233216.06|  7|    23|       0.0|car17| 94805.61|135000.0|2020|\n",
      "|350707.38|  8|     6|  39884.39|car20| 64410.62|125000.0|2020|\n",
      "| 20768.06|  9|    12|       0.0|car19|128298.82|135000.0|2020|\n",
      "|293106.66| 10|     6|       0.0| car8|100806.92|135000.0|2020|\n",
      "|455015.34|  2|     2|       0.0|car10|160893.77|179000.0|2020|\n",
      "| 99629.62|  4|    30|  52593.63| car9|  20000.0|185000.0|2020|\n",
      "|122025.08|  3|    22|       0.0| car1|119159.65|145000.0|2020|\n",
      "+---------+---+------+----------+-----+---------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "economyFg.read().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the timeline metadata we can inspect the value of a table at a specific point in time, as well as pull changes incrementally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commitTimeline: java.util.Map[String,java.util.Map[String,String]] = {1604795370000={committedOn=20201108002930, rowsUpdated=2, rowsDeleted=0, rowsInserted=3}, 1604795225000={committedOn=20201108002705, rowsUpdated=0, rowsDeleted=0, rowsInserted=4}, 1604797033000={committedOn=20201108005713, rowsUpdated=0, rowsDeleted=0, rowsInserted=3}}\n",
      "(1604795370000,{committedOn=20201108002930, rowsUpdated=2, rowsDeleted=0, rowsInserted=3})\n",
      "(1604795225000,{committedOn=20201108002705, rowsUpdated=0, rowsDeleted=0, rowsInserted=4})\n",
      "(1604797033000,{committedOn=20201108005713, rowsUpdated=0, rowsDeleted=0, rowsInserted=3})\n"
     ]
    }
   ],
   "source": [
    "val commitTimeline = economyFg.commitDetails()\n",
    "for ((k,v) <- commitTimeline){\n",
    "    println (k,v)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+----------+-----+---------+--------+----+\n",
      "|     loan| id|hyears|commission|  car|   salary|  hvalue|year|\n",
      "+---------+---+------+----------+-----+---------+--------+----+\n",
      "|395015.34|  2|     2|       0.0|car20|140893.77|135000.0|2020|\n",
      "| 354724.2|  1|    30|       0.0|car15|110499.73|235000.0|2020|\n",
      "| 99629.62|  4|    30|  52593.63| car9|  20000.0|185000.0|2020|\n",
      "|122025.08|  3|    22|       0.0| car1|119159.65|145000.0|2020|\n",
      "+---------+---+------+----------+-----+---------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// pull 1st commit\n",
    "economyFg.read(\"2020-11-08 00:27:05\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+----------+-----+---------+--------+----+\n",
      "|     loan| id|hyears|commission|  car|   salary|  hvalue|year|\n",
      "+---------+---+------+----------+-----+---------+--------+----+\n",
      "| 564724.2|  1|    30|       0.0|car17|120499.73|205000.0|2020|\n",
      "| 458679.8|  5|     1|       0.0|car15| 93956.32|135000.0|2020|\n",
      "| 216839.7|  6|    19|  52809.15| car7| 41365.43|135000.0|2020|\n",
      "|233216.06|  7|    23|       0.0|car17| 94805.61|135000.0|2020|\n",
      "|455015.34|  2|     2|       0.0|car10|160893.77|179000.0|2020|\n",
      "| 99629.62|  4|    30|  52593.63| car9|  20000.0|185000.0|2020|\n",
      "|122025.08|  3|    22|       0.0| car1|119159.65|145000.0|2020|\n",
      "+---------+---+------+----------+-----+---------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// pull 2nd commit\n",
    "economyFg.read(\"2020-11-08 00:29:30\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+----------+-----+---------+--------+----+\n",
      "|     loan| id|hyears|commission|  car|   salary|  hvalue|year|\n",
      "+---------+---+------+----------+-----+---------+--------+----+\n",
      "| 564724.2|  1|    30|       0.0|car17|120499.73|205000.0|2020|\n",
      "| 458679.8|  5|     1|       0.0|car15| 93956.32|135000.0|2020|\n",
      "| 216839.7|  6|    19|  52809.15| car7| 41365.43|135000.0|2020|\n",
      "|233216.06|  7|    23|       0.0|car17| 94805.61|135000.0|2020|\n",
      "|350707.38|  8|     6|  39884.39|car20| 64410.62|125000.0|2020|\n",
      "| 20768.06|  9|    12|       0.0|car19|128298.82|135000.0|2020|\n",
      "|293106.66| 10|     6|       0.0| car8|100806.92|135000.0|2020|\n",
      "|455015.34|  2|     2|       0.0|car10|160893.77|179000.0|2020|\n",
      "| 99629.62|  4|    30|  52593.63| car9|  20000.0|185000.0|2020|\n",
      "|122025.08|  3|    22|       0.0| car1|119159.65|145000.0|2020|\n",
      "+---------+---+------+----------+-----+---------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// pull 3rd commit\n",
    "economyFg.read(\"2020-11-08 00:57:13\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hopsworks Feature Store also provides a method for incremental reads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+----------+-----+---------+--------+----+\n",
      "|     loan| id|hyears|commission|  car|   salary|  hvalue|year|\n",
      "+---------+---+------+----------+-----+---------+--------+----+\n",
      "| 564724.2|  1|    30|       0.0|car17|120499.73|205000.0|2020|\n",
      "| 458679.8|  5|     1|       0.0|car15| 93956.32|135000.0|2020|\n",
      "| 216839.7|  6|    19|  52809.15| car7| 41365.43|135000.0|2020|\n",
      "|233216.06|  7|    23|       0.0|car17| 94805.61|135000.0|2020|\n",
      "|455015.34|  2|     2|       0.0|car10|160893.77|179000.0|2020|\n",
      "+---------+---+------+----------+-----+---------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Pull changes that happened between the first and second commits\n",
    "economyFg.readChanges(\"2020-11-08 00:27:05\", \"2020-11-08 00:29:30\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+----------+-----+---------+--------+----+\n",
      "|     loan| id|hyears|commission|  car|   salary|  hvalue|year|\n",
      "+---------+---+------+----------+-----+---------+--------+----+\n",
      "|350707.38|  8|     6|  39884.39|car20| 64410.62|125000.0|2020|\n",
      "| 20768.06|  9|    12|       0.0|car19|128298.82|135000.0|2020|\n",
      "|293106.66| 10|     6|       0.0| car8|100806.92|135000.0|2020|\n",
      "+---------+---+------+----------+-----+---------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Pull changes that happened between the second and third commits \n",
    "economyFg.readChanges(\"2020-11-08 00:29:30\", \"2020-11-08 00:57:13\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join Feature groups that correspond to specific point in time\n",
    "If we are interetsted to join Feature groups all of them correspong to one specific point in time then we can issue `asOf` method on join `Query` object.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joined_features: com.logicalclocks.hsfs.metadata.Query = SELECT `fg0`.`loan`, `fg0`.`id`, `fg0`.`hyears`, `fg0`.`commission`, `fg0`.`car`, `fg0`.`salary`, `fg0`.`hvalue`, `fg0`.`year`, `fg1`.`elevel`, `fg1`.`age`, `fg1`.`id`, `fg1`.`zipcode`, `fg2`.`id`, `fg2`.`class`, `fg2`.`year` FROM `fg0` `fg0` INNER JOIN `fg1` `fg1` ON `fg0`.`id` = `fg1`.`id` INNER JOIN `fg2` `fg2` ON `fg0`.`id` = `fg2`.`id`\n"
     ]
    }
   ],
   "source": [
    "val joined_features = ((economyFg.selectAll())\n",
    "                   .join(demographyFg.selectAll(), Seq(\"id\"), JoinType.INNER)\n",
    "                   .join(classFg.selectAll(), Seq(\"id\"), JoinType.INNER)\n",
    "                   .asOf(\"2020-11-08 01:00:00\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+----------+-----+---------+--------+----+------+---+---+--------+---+------+----+\n",
      "|     loan| id|hyears|commission|  car|   salary|  hvalue|year|elevel|age| id| zipcode| id| class|year|\n",
      "+---------+---+------+----------+-----+---------+--------+----+------+---+---+--------+---+------+----+\n",
      "| 564724.2|  1|    30|       0.0|car17|120499.73|205000.0|2020|level3| 54|  1|zipcode5|  1|groupA|2020|\n",
      "| 216839.7|  6|    19|  52809.15| car7| 41365.43|135000.0|2020|level2| 71|  6|zipcode3|  6|groupA|2020|\n",
      "|122025.08|  3|    22|       0.0| car1|119159.65|145000.0|2020|level2| 49|  3|zipcode4|  3|groupB|2020|\n",
      "| 458679.8|  5|     1|       0.0|car15| 93956.32|135000.0|2020|level1| 59|  5|zipcode2|  5|groupA|2020|\n",
      "| 99629.62|  4|    30|  52593.63| car9|  20000.0|185000.0|2020|level0| 56|  4|zipcode2|  4|groupB|2020|\n",
      "|233216.06|  7|    23|       0.0|car17| 94805.61|135000.0|2020|level1| 32|  7|zipcode2|  7|groupA|2020|\n",
      "|455015.34|  2|     2|       0.0|car10|160893.77|179000.0|2020|level1| 44|  2|zipcode8|  2|groupB|2020|\n",
      "+---------+---+------+----------+-----+---------+--------+----+------+---+---+--------+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_features.read().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join Feature groups that correspond to different points in time\n",
    "Hopswork Feature store also provides functionality to join Feature groups that correspond to different points in time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "economyFgQuery: com.logicalclocks.hsfs.metadata.Query = SELECT `fg0`.`loan`, `fg0`.`id`, `fg0`.`hyears`, `fg0`.`commission`, `fg0`.`car`, `fg0`.`salary`, `fg0`.`hvalue`, `fg0`.`year` FROM `fg0` `fg0`\n",
      "demographyFgQuery: com.logicalclocks.hsfs.metadata.Query = SELECT `fg0`.`elevel`, `fg0`.`age`, `fg0`.`id`, `fg0`.`zipcode` FROM `fg0` `fg0`\n",
      "classFgQuery: com.logicalclocks.hsfs.metadata.Query = SELECT `fg0`.`id`, `fg0`.`class`, `fg0`.`year` FROM `fg0` `fg0`\n"
     ]
    }
   ],
   "source": [
    "val economyFgQuery = economyFg.selectAll().asOf(\"2020-11-08 00:57:13\")\n",
    "val demographyFgQuery = demographyFg.selectAll().asOf(\"2020-11-08-01:01:17\")\n",
    "val classFgQuery =  classFg.selectAll().asOf(\"2020-11-08-01:01:17\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joined_features: com.logicalclocks.hsfs.metadata.Query = SELECT `fg0`.`loan`, `fg0`.`id`, `fg0`.`hyears`, `fg0`.`commission`, `fg0`.`car`, `fg0`.`salary`, `fg0`.`hvalue`, `fg0`.`year`, `fg1`.`elevel`, `fg1`.`age`, `fg1`.`id`, `fg1`.`zipcode`, `fg2`.`id`, `fg2`.`class`, `fg2`.`year` FROM `fg0` `fg0` INNER JOIN `fg1` `fg1` ON `fg0`.`id` = `fg1`.`id` INNER JOIN `fg2` `fg2` ON `fg0`.`id` = `fg2`.`id`\n"
     ]
    }
   ],
   "source": [
    "val joined_features = economyFgQuery.join(demographyFgQuery, Seq(\"id\"), JoinType.INNER).join(classFgQuery, Seq(\"id\"), JoinType.INNER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+----------+-----+---------+--------+----+------+---+---+--------+---+------+----+\n",
      "|     loan| id|hyears|commission|  car|   salary|  hvalue|year|elevel|age| id| zipcode| id| class|year|\n",
      "+---------+---+------+----------+-----+---------+--------+----+------+---+---+--------+---+------+----+\n",
      "| 564724.2|  1|    30|       0.0|car17|120499.73|205000.0|2020|level3| 54|  1|zipcode5|  1|groupA|2020|\n",
      "| 216839.7|  6|    19|  52809.15| car7| 41365.43|135000.0|2020|level2| 71|  6|zipcode3|  6|groupA|2020|\n",
      "|122025.08|  3|    22|       0.0| car1|119159.65|145000.0|2020|level2| 49|  3|zipcode4|  3|groupB|2020|\n",
      "| 458679.8|  5|     1|       0.0|car15| 93956.32|135000.0|2020|level1| 59|  5|zipcode2|  5|groupA|2020|\n",
      "| 99629.62|  4|    30|  52593.63| car9|  20000.0|185000.0|2020|level0| 56|  4|zipcode2|  4|groupB|2020|\n",
      "|233216.06|  7|    23|       0.0|car17| 94805.61|135000.0|2020|level1| 32|  7|zipcode2|  7|groupA|2020|\n",
      "|455015.34|  2|     2|       0.0|car10|160893.77|179000.0|2020|level1| 44|  2|zipcode8|  2|groupB|2020|\n",
      "+---------+---+------+----------+-----+---------+--------+----+------+---+---+--------+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_features.read().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}