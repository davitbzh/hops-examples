{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>0</td><td>application_1613683661894_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hopsworks0.logicalclocks.com:8088/proxy/application_1613683661894_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://hopsworks0.logicalclocks.com:8042/node/containerlogs/container_1613683661894_0002_01_000001/demo_fs_meb10000__meb10000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<pyspark.sql.session.SparkSession object at 0x7ff87cd27f10>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare feature groups for training sales_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering\n",
    "\n",
    "We are going to use a dataset containing information related to a chain of deparment stores. The dataset is taken from [Kaggke](https://www.kaggle.com/manjeetsingh/retaildataset?select=Features+data+set.csv).\n",
    "\n",
    "We are going to create 3 feature groups:\n",
    "- `stores_fg`: it's going to contain features related to the store itself. Mainly the category, the number of deparmetns and the size.\n",
    "- `sales_fg`: it's going to contain sales features for each store/deparment over the weeks. \n",
    "- `exogenous_fg`: it's going to contain features which are not related to the stores themselves, but they have an effect on sales. These features are, for instance, the gas price, the unemployment rate, temperature in the area and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import hdfs\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "stores_csv = spark.read\\\n",
    "             .option(\"inferSchema\", \"true\")\\\n",
    "             .option(\"header\", \"true\")\\\n",
    "             .format(\"csv\")\\\n",
    "             .load(\"hdfs:///Projects/{}/Jupyter/hsfs/archive/stores data-set.csv\".format(hdfs.project_name()))\n",
    "\n",
    "exogenous_csv = spark.read\\\n",
    "             .option(\"inferSchema\", \"true\")\\\n",
    "             .option(\"header\", \"true\")\\\n",
    "             .format(\"csv\")\\\n",
    "             .load(\"hdfs:///Projects/{}/Jupyter/hsfs/archive/Features data set.csv\".format(hdfs.project_name()))\n",
    "\n",
    "sales_csv = spark.read\\\n",
    "             .option(\"inferSchema\", \"true\")\\\n",
    "             .option(\"header\", \"true\")\\\n",
    "             .format(\"csv\")\\\n",
    "             .load(\"hdfs:///Projects/{}/Jupyter/hsfs/archive/sales data-set.csv\".format(hdfs.project_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_depts_count = stores_csv\\\n",
    "                    .join(sales_csv, \"store\")\\\n",
    "                    .groupBy(\"store\")\\\n",
    "                    .agg(F.countDistinct(\"dept\"))\\\n",
    "                    .withColumnRenamed(\"count(DISTINCT dept)\", \"num_depts\")\n",
    "\n",
    "stores_fg = stores_csv\\\n",
    "            .join(stores_depts_count, \"store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create `store_fg` feature group\n",
    "\n",
    "Create a feature group named `store_fg`. The store is the primary key uniquely identifying all the remaining features in this feature group. Note that online feature store must be enabled by `online_enabled=True` to be able to retrieve featues of online serving. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_fg_meta = fs.create_feature_group(name=\"store_fg\",\n",
    "                                       version=1,\n",
    "                                       primary_key=['store'],\n",
    "                                       description=\"Store related features\",\n",
    "                                       time_travel_format=None,\n",
    "                                       online_enabled=True, \n",
    "                                       statistics_config={\"enabled\": True, \"histograms\": True, \"correlations\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.feature_group.FeatureGroup object at 0x7ff82f78c050>"
     ]
    }
   ],
   "source": [
    "store_fg_meta.save(stores_fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "days = lambda i: i * 86400 \n",
    "\n",
    "sales_df = sales_csv.withColumn('date', F.to_date(\"date\", 'dd/MM/yyy'))\\\n",
    "                    .withColumn('timestamp', F.unix_timestamp(\"date\"))\n",
    "\n",
    "# Define aggregation window to compute sales performances over the past period of time\n",
    "last_month_window_store_dep = Window.partitionBy(['store', 'dept']).orderBy(F.col(\"timestamp\").cast(\"long\")).rangeBetween(days(-30), days(-1))\n",
    "last_quarter_window_store_dep = Window.partitionBy(['store', 'dept']).orderBy(F.col(\"timestamp\").cast(\"long\")).rangeBetween(days(-90), days(-1))\n",
    "last_six_month_window_store_dep = Window.partitionBy(['store', 'dept']).orderBy(F.col(\"timestamp\").cast(\"long\")).rangeBetween(days(-180), days(-1))\n",
    "last_year_window_store_dep = Window.partitionBy(['store', 'dept']).orderBy(F.col(\"timestamp\").cast(\"long\")).rangeBetween(days(-365), days(-1))\n",
    "\n",
    "last_month_window_store = Window.partitionBy('store').orderBy(F.col(\"timestamp\").cast(\"long\")).rangeBetween(days(-30), days(-1))\n",
    "last_quarter_window_store = Window.partitionBy('store').orderBy(F.col(\"timestamp\").cast(\"long\")).rangeBetween(days(-90), days(-1))\n",
    "last_six_month_window_store = Window.partitionBy('store').orderBy(F.col(\"timestamp\").cast(\"long\")).rangeBetween(days(-180), days(-1))\n",
    "last_year_window_store = Window.partitionBy('store').orderBy(F.col(\"timestamp\").cast(\"long\")).rangeBetween(days(-365), days(-1))\n",
    "\n",
    "# Build feature group dataframe\n",
    "sales_fg = sales_df.withColumn(\"sales_last_month_store_dep\", F.sum(\"weekly_sales\").over(last_month_window_store_dep))\\\n",
    "        .withColumn(\"sales_last_quarter_store_dep\", F.sum(\"weekly_sales\").over(last_quarter_window_store_dep))\\\n",
    "        .withColumn(\"sales_last_six_month_store_dep\", F.sum(\"weekly_sales\").over(last_six_month_window_store_dep))\\\n",
    "        .withColumn(\"sales_last_year_store_dep\", F.sum(\"weekly_sales\").over(last_year_window_store_dep))\\\n",
    "        .withColumn(\"sales_last_month_store\", F.sum(\"weekly_sales\").over(last_month_window_store))\\\n",
    "        .withColumn(\"sales_last_quarter_store\", F.sum(\"weekly_sales\").over(last_quarter_window_store))\\\n",
    "        .withColumn(\"sales_last_six_month_store\", F.sum(\"weekly_sales\").over(last_six_month_window_store))\\\n",
    "        .withColumn(\"sales_last_year_store\", F.sum(\"weekly_sales\").over(last_year_window_store))\\\n",
    "        .drop(\"timestamp\")\\\n",
    "        .fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create `sales_fg` feature group\n",
    "\n",
    "Differently from the `store_fg`, for the `sales_fg` we are going to define a composite primary key. This means that each entry in the `sales_fg` is going to be uniquely identified by the store, the department and the week. In this case we are going to specify also a partition key. Partitioning is a tool available at your disposal to improve the performances of querying a feature group. Note that online feature store must be enabled by `online_enabled=True` to be able to retrieve featues of online serving. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_fg_meta = fs.create_feature_group(name=\"sales_fg\",\n",
    "                                        version=1,\n",
    "                                        primary_key=['store', 'dept', 'date'],\n",
    "                                        description=\"Sales related features\",\n",
    "                                        time_travel_format=None,  \n",
    "                                        online_enabled=True,                                         \n",
    "                                        statistics_config=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.feature_group.FeatureGroup object at 0x7ff82fa11ad0>"
     ]
    }
   ],
   "source": [
    "sales_fg_meta.save(sales_fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering Exogenous features\n",
    "\n",
    "This feature group will contain exogenous features that can influence sales, but are not under the control of the distribution chain. These are the unemployment, the consumer price index (cpi) and so on.\n",
    "We are going to write these features as they are in the feature store. Note that online feature store must be enabled by `online_enabled=True` to be able to retrieve featues of online serving. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.feature_group.FeatureGroup object at 0x7ff82fa11e90>"
     ]
    }
   ],
   "source": [
    "exogenous_fg = exogenous_csv.withColumn('date', F.to_date(\"date\", 'dd/MM/yyy'))\n",
    "\n",
    "exogenous_fg_meta = fs.create_feature_group(name=\"exogenous_fg\",\n",
    "                                            version=1,\n",
    "                                            primary_key=['store', 'date'],\n",
    "                                            description=\"External features that influence sales, but are not under the control of the distribution chain\",\n",
    "                                            time_travel_format=None,   \n",
    "                                            online_enabled=True,                                                                                     \n",
    "                                            statistics_config={\"enabled\": True, \"histograms\": True, \"correlations\": True})\n",
    "exogenous_fg_meta.save(exogenous_fg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training dataset using query object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_fg_meta = fs.get_feature_group(\"sales_fg\",1)\n",
    "store_fg_meta = fs.get_feature_group(\"store_fg\",1)\n",
    "exogenous_fg_meta = fs.get_feature_group(\"exogenous_fg\",1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = sales_fg_meta.select([\"weekly_sales\", \"sales_last_month_store\", \"sales_last_quarter_store\", \n",
    "                         \"sales_last_year_store_dep\", \"sales_last_month_store_dep\", \"sales_last_quarter_store_dep\", \n",
    "                         \"sales_last_six_month_store_dep\", \"sales_last_six_month_store\", \"sales_last_year_store\"])\\\n",
    "                .join(store_fg_meta.select([\"num_depts\", \"size\"]))\\\n",
    "                .join(exogenous_fg_meta.select(['fuel_price']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.training_dataset.TrainingDataset object at 0x7ff82f86a150>"
     ]
    }
   ],
   "source": [
    "from hsfs.storage_connector import StorageConnector\n",
    "td = fs.create_training_dataset(name=\"sales_model\",\n",
    "                               description=\"Dataset to train the sales model\",\n",
    "                               data_format=\"tfrecord\",\n",
    "                               label = [\"weekly_sales\"],                                \n",
    "                               version=1)\n",
    "\n",
    "td.save(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Tensorflow Keras model and save with SavedModel format\n",
    "---\n",
    "\n",
    "<font color='red'> <h3>Tested with TensorFlow 2.4.0</h3></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve traing dataset from feature store and prepare tf data input\n",
    "### Input the training dataset to a model training loop\n",
    "If you are training a model, HSFS provides `tf_data` method that returns `TFDataEngine` object with utility methods to read training dataset as `tf.data.Dataset` object to read the training dataset and feed it to a model training loop efficiently. \n",
    "* Currently `TFDataEngine` provides 2 utility methods `tf_record_dataset` and `tf_csv_dataset` for reading `.tfrecord` and `.csv` files, respectivelly.\n",
    "* Both methods support only following feature types `string`, `short`, `int`, `long`, `float` and `double`.\n",
    "* In both methods you can set `process` argument to `True` and they will return `PrefetchDataset` ready to input to model training loop.\n",
    "* If you would like to apply your own logic to feature transformation using `tf.data.Dataset` then set `process` argument to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = fs.get_training_dataset(\"sales_model\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = td.tf_data(target_name=td.label[0], is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<h1>Machine Learning on <a href=\"https://github.com/logicalclocks/hopsworks\">Hopsworks\n",
    "</a></h1> \n",
    "</p>\n",
    "\n",
    "## The `hops` python module\n",
    "\n",
    "`hops` is a helper library for Hops that facilitates development by hiding the complexity of running applications and iteracting with services.\n",
    "\n",
    "Have a feature request or encountered an issue? Please let us know on <a href=\"https://github.com/logicalclocks/hops-util-py\">github</a>.\n",
    "\n",
    "### Using the `experiment` module\n",
    "\n",
    "To be able to run your Machine Learning code in Hopsworks, the code for the whole program needs to be provided and put inside a wrapper function. Everything, from importing libraries to reading data and defining the model and running the program needs to be put inside a wrapper function.\n",
    "\n",
    "The `experiment` module provides an api to Python programs such as TensorFlow, Keras and PyTorch on a Hopsworks on any number of machines and GPUs.\n",
    "\n",
    "An Experiment could be a single Python program, which we refer to as an **Experiment**. \n",
    "\n",
    "Grid search or genetic hyperparameter optimization such as differential evolution which runs several Experiments in parallel, which we refer to as **Parallel Experiment**. \n",
    "\n",
    "ParameterServerStrategy, CollectiveAllReduceStrategy and MultiworkerMirroredStrategy making multi-machine/multi-gpu training as simple as invoking a function for orchestration. This mode is referred to as **Distributed Training**.\n",
    "\n",
    "### Using the `tensorboard` module\n",
    "The `tensorboard` module allow us to get the log directory for summaries and checkpoints to be written to the TensorBoard we will see in a bit. The only function that we currently need to call is `tensorboard.logdir()`, which returns the path to the TensorBoard log directory. Furthermore, the content of this directory will be put in as a Dataset in your project's Experiments folder.\n",
    "\n",
    "The directory could in practice be used to store other data that should be accessible after the experiment is finished.\n",
    "```python\n",
    "# Use this module to get the TensorBoard logdir\n",
    "from hops import tensorboard\n",
    "tensorboard_logdir = tensorboard.logdir()\n",
    "```\n",
    "\n",
    "### Using the `hdfs` module\n",
    "The `hdfs` module provides a method to get the path in HopsFS where your data is stored, namely by calling `hdfs.project_path()`. The path resolves to the root path for your project, which is the view that you see when you click `Data Sets` in HopsWorks. To point where your actual data resides in the project you to append the full path from there to your Dataset. For example if you create a salse folder in your Resources Dataset, the path to the salse data would be `hdfs.project_path() + 'Resources/sales'`\n",
    "\n",
    "```python\n",
    "# Use this module to get the path to your project in HopsFS, then append the path to your Dataset in your project\n",
    "from hops import hdfs\n",
    "project_path = hdfs.project_path()\n",
    "```\n",
    "\n",
    "```python\n",
    "# Downloading the sales dataset to the current working directory\n",
    "from hops import hdfs\n",
    "sales_hdfs_path = hdfs.project_path() + \"Resources/sales\"\n",
    "local_sales_path = hdfs.copy_to_local(sales_hdfs_path)\n",
    "```\n",
    "\n",
    "### Documentation\n",
    "See the following links to learn more about running experiments in Hopsworks\n",
    "\n",
    "- <a href=\"https://hopsworks.readthedocs.io/en/latest/hopsml/experiment.html\">Learn more about experiments</a>\n",
    "<br>\n",
    "- <a href=\"https://hopsworks.readthedocs.io/en/latest/hopsml/hopsML.html\">Building End-To-End pipelines</a>\n",
    "<br>\n",
    "- Give us a star, create an issue or a feature request on  <a href=\"https://github.com/logicalclocks/hopsworks\">Hopsworks github</a>\n",
    "\n",
    "### Managing experiments\n",
    "Experiments service provides a unified view of all the experiments run using the `experiment` module.\n",
    "<br>\n",
    "As demonstrated in the gif it provides general information about the experiment and the resulting metric. Experiments can be visualized meanwhile or after training in a TensorBoard.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_sales_model():\n",
    "    \n",
    "    import os\n",
    "    import sys\n",
    "    import uuid\n",
    "    import random\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    from tensorflow import keras\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout\n",
    "    from tensorflow.keras.callbacks import TensorBoard\n",
    "    from tensorflow.keras import backend as K\n",
    "\n",
    "    import math\n",
    "    from hops import tensorboard\n",
    "\n",
    "    from hops import model as hops_model\n",
    "    from hops import hdfs\n",
    "\n",
    "    import pydoop.hdfs as pydoop\n",
    "\n",
    "    train_input_processed = train_input.tf_record_dataset(process=True, batch_size =32, num_epochs=1)\n",
    "    \n",
    "    # Define a Keras Model.\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(11, activation='relu', input_shape=(11,)))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "    # Compile the model.\n",
    "    model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "                  optimizer= tf.keras.optimizers.Adam(0.001))\n",
    "        \n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.TensorBoard(log_dir=tensorboard.logdir()),\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=tensorboard.logdir()),\n",
    "    ]\n",
    "    model.fit(train_input_processed, \n",
    "        verbose=0,\n",
    "        epochs=5, \n",
    "        steps_per_epoch=5,\n",
    "        validation_data=train_input_processed,\n",
    "        validation_steps=1,                    \n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    score = model.evaluate(train_input_processed, steps=1)\n",
    "\n",
    "    # Export model\n",
    "    # WARNING(break-tutorial-inline-code): The following code snippet is\n",
    "    # in-lined in tutorials, please update tutorial documents accordingly\n",
    "    # whenever code changes.\n",
    "\n",
    "    export_path = os.getcwd() + '/model-' + str(uuid.uuid4())\n",
    "    print('Exporting trained model to: {}'.format(export_path))\n",
    "    \n",
    "    tf.saved_model.save(model, export_path)\n",
    "\n",
    "    print('Done exporting!')\n",
    "    \n",
    "    metrics = {'loss': score}\n",
    "    \n",
    "    hops_model.export(export_path, \"sales\", metrics=metrics)    \n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import experiment\n",
    "from hops import hdfs\n",
    "\n",
    "experiment.launch(keras_sales_model, name='sales', local_logdir=True, metric_key='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Model Repository for best sales Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import model\n",
    "from hops.model import Metric\n",
    "MODEL_NAME=\"sales\"\n",
    "EVALUATION_METRIC=\"loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model.get_best_model(MODEL_NAME, EVALUATION_METRIC, Metric.MIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model name: ' + best_model['name'])\n",
    "print('Model version: ' + str(best_model['version']))\n",
    "print(best_model['metrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model Serving of Exported Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create serving\n",
    "model_path=\"/Models/\" + best_model['name']\n",
    "response = serving.create_or_update(model_path, MODEL_NAME, serving_type=\"TENSORFLOW\", \n",
    "                                 model_version=best_model['version'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available servings in the project\n",
    "for s in serving.get_all():\n",
    "    print(s.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get serving status\n",
    "serving.get_status(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Model Serving for active servings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Model Serving Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if serving.get_status(MODEL_NAME) == 'Stopped':\n",
    "    serving.start(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "while serving.get_status(MODEL_NAME) != \"Running\":\n",
    "    time.sleep(5) # Let the serving startup correctly\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hsfs TrainingDataset object provides utility method `get_serving_vector` to retrieve serving vector from online feature store. This method assumes that all feature groups used to creates this training dataset are online enabled. \n",
    "##### `get_serving_vector` method expect dict object where keys are feature primary key names. To identify with primary key names are used for this training dataset query use `serving_keys` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = fs.get_training_dataset(\"sales_model\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'store', 'date', 'dept'}"
     ]
    }
   ],
   "source": [
    "td.init_prepared_statement() #this is need to get serving_keys, however not necessary for `get_serving_vector` method\n",
    "td.serving_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For demo purposes lets prepare list of primary key values that we are interest to get model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "incoming_data = [(31,\"2010-02-05\",47),\n",
    "                 (2,\"2010-02-12\",92),\n",
    "                 (20,\"2010-03-05\",11),\n",
    "                 (4,\"2010-04-02\",52),\n",
    "                 (12,\"2010-05-07\",27)\n",
    "                ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send Prediction Requests to the Served Model using Hopsworks REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_NAME = serving.get_kafka_topic(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iterate over incoming_data and use `td.get_serving_vector` to retrieve serving vector for each primary key combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [[-42484.6484]]}\n",
      "{'predictions': [[-649627.875]]}\n",
      "{'predictions': [[-2381425.5]]}\n",
      "{'predictions': [[-7573677.0]]}\n",
      "{'predictions': [[-6883642.0]]}"
     ]
    }
   ],
   "source": [
    "import json\n",
    "for i in incoming_data:\n",
    "    serving_vector = td.get_serving_vector({'store': i[0],'date': i[1], 'dept': i[2]})\n",
    "    data = {\n",
    "                \"signature_name\": \"serving_default\", \"instances\": [serving_vector]\n",
    "            }\n",
    "    response = serving.make_inference_request(MODEL_NAME, data)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}