{"cells":[{"cell_type":"markdown","source":["# Feature Store Quick Start\n\nThis notebook gives you a quick overview of how you can intergrate the Feature Store on Hopsworks with Databricks and S3. We'll go over four steps:\n\n1. Generate some sample data and store it on S3\n2. Do some feature engineering with Databricks and the data from S3\n3. **Save the engineered features to the Feature Store**\n4. **Select a group of the features from the Feature Store and create a training dataset of tf records stored on S3**\n\n**This requires a Hopsworks instance that is available from Databricks, see [Databricks Quick Start](https://hopsworks.readthedocs.io/en/latest/getting_started/hopsworksai/guides/databricks_quick_start.html).**"],"metadata":{}},{"cell_type":"markdown","source":["## Imports\n\nWe'll use numpy and pandas for data generation, pyspark for feature engineering, tensorflow and keras for model training, and the hops `featurestore` library for interacting with the feature store."],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nimport random\nimport pandas as pd\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\nfrom pyspark.sql import Row\nfrom hops import featurestore\nREGION='us-west-2'\nSECRETS_STORE='secretsmanager'\nPROJECT='demo_featurestore_jim00000'\nFEATURE_STORE_HOST='4d20b970-c52a-11ea-a485-f907f59af45e.aws.hopsworks.ai'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ModuleNotFoundError</span>                       Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-847864287059200&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> sqlContext <span class=\"ansi-blue-fg\">=</span> SQLContext<span class=\"ansi-blue-fg\">(</span>sc<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> <span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql <span class=\"ansi-green-fg\">import</span> Row\n<span class=\"ansi-green-fg\">----&gt; 7</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">from</span> hops <span class=\"ansi-green-fg\">import</span> featurestore\n<span class=\"ansi-green-intense-fg ansi-bold\">      8</span> REGION<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;us-west-2&#39;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      9</span> SECRETS_STORE<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;secretsmanager&#39;</span>\n\n<span class=\"ansi-red-fg\">ModuleNotFoundError</span>: No module named &#39;hops&#39;</div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["## Setting up the Feature Store Connector"],"metadata":{}},{"cell_type":"code","source":["# setup_databricks only needs to be executed onces per cluster\n# Check the documentation for details: https://hopsworks.readthedocs.io/en/latest/getting_started/hopsworksai/guides/databricks_quick_start.html#id8\nfeaturestore.setup_databricks(\n  FEATURE_STORE_HOST,\n  PROJECT,\n  region_name=REGION,\n  secrets_store=SECRETS_STORE,\n  hostname_verification=True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\n    In the advanced options of your databricks cluster configuration \n    add the following path to Init Scripts: dbfs:/hops/scripts/initScript.sh\n\n    add the following to the Spark Config:\n    spark.hadoop.fs.hopsfs.impl io.hops.hopsfs.client.HopsFileSystem\n    spark.hadoop.hops.ipc.server.ssl.enabled true\n    spark.hadoop.hops.ssl.hostname.verifier ALLOW_ALL\n    spark.hadoop.hops.rpc.socket.factory.class.default io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory\n    spark.hadoop.client.rpc.ssl.enabled.protocol TLSv1.2\n    spark.hadoop.hops.ssl.keystores.passwd.name /dbfs/hops/material_passwd\n    spark.hadoop.hops.ssl.keystore.name /dbfs/hops/keyStore.jks\n    spark.hadoop.hops.ssl.trustore.name /dbfs/hops/trustStore.jks\n    spark.sql.hive.metastore.jars /tmp/apache-hive-bin/lib/*\n    spark.hadoop.hive.metastore.uris thrift://ip-10-199-254-9.us-west-2.compute.internal:9083\n\n    Then save and restart the cluster.\n    \n</div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["## Connecting to the Feature Store"],"metadata":{}},{"cell_type":"code","source":["# Connect to the feature store, see https://hopsworks.readthedocs.io/en/latest/getting_started/hopsworksai/guides/databricks_quick_start.html#step-4-3-connecting-to-the-feature-store\nfeaturestore.connect(\n  FEATURE_STORE_HOST,\n  PROJECT,\n  region_name=REGION,\n  secrets_store=SECRETS_STORE,\n  hostname_verification=True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["## Mounting an S3 bucket to Databricks"],"metadata":{}},{"cell_type":"code","source":["# Mount a bucket so that we can simulate a Datalake based on S3\n# This requires IAM roles to be set up for Databricks, see https://docs.databricks.com/data/data-sources/aws/amazon-s3.html#access-s3-buckets-directly\nAWS_BUCKET_NAME = \"jim-oregan\" # Ensure to replace with your bucket\nMOUNT_NAME = \"/mnt/demo_training_datasets\"\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["dbutils.fs.mount(\"s3a://%s\" % AWS_BUCKET_NAME, MOUNT_NAME)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ExecutionError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-707875173316342&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>dbutils<span class=\"ansi-blue-fg\">.</span>fs<span class=\"ansi-blue-fg\">.</span>mount<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;s3a://%s&#34;</span> <span class=\"ansi-blue-fg\">%</span> AWS_BUCKET_NAME<span class=\"ansi-blue-fg\">,</span> MOUNT_NAME<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/local_disk0/tmp/1588254532465-0/dbutils.py</span> in <span class=\"ansi-cyan-fg\">f_with_exception_handling</span><span class=\"ansi-blue-fg\">(*args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    312</span>                     exc<span class=\"ansi-blue-fg\">.</span>__context__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    313</span>                     exc<span class=\"ansi-blue-fg\">.</span>__cause__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-fg\">--&gt; 314</span><span class=\"ansi-red-fg\">                     </span><span class=\"ansi-green-fg\">raise</span> exc\n<span class=\"ansi-green-intense-fg ansi-bold\">    315</span>             <span class=\"ansi-green-fg\">return</span> f_with_exception_handling\n<span class=\"ansi-green-intense-fg ansi-bold\">    316</span> \n\n<span class=\"ansi-red-fg\">ExecutionError</span>: An error occurred while calling o292.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/demo_training_datasets; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/demo_training_datasets\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:123)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:63)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:465)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/demo_training_datasets\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:224)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:326)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:220)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:79)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:103)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:102)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:102)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$3.applyOrElse(DbfsServerBackend.scala:304)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$3.applyOrElse(DbfsServerBackend.scala:282)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:51)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:78)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:78)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:47)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:428)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:230)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:14)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:268)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:14)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:409)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:336)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:14)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:46)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:611)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:611)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:534)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$4(JettyServer.scala:321)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:230)\n\tat com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:152)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:268)\n\tat com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:152)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:310)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:217)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\t... 1 more\n</div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["## Generate Sample Data\n\nLets generate two sample datasets and store them on S3:\n\n1. `houses_for_sale_data`:\n\n```bash\n+-------+--------+------------------+------------------+------------------+\n|area_id|house_id|       house_worth|         house_age|        house_size|\n+-------+--------+------------------+------------------+------------------+\n|      1|       0| 11678.15482418699|133.88670106643886|366.80067322738535|\n|      1|       1| 2290.436167500643|15994.969706808222|195.84014889823976|\n|      1|       2| 8380.774578431328|1994.8576926471007|1544.5164614303735|\n|      1|       3|11641.224696102923|23104.501275562343|1673.7222604337876|\n|      1|       4| 5382.089422436954| 13903.43637058141| 274.2912104765028|\n+-------+--------+------------------+------------------+------------------+\n\n |-- area_id: long (nullable = true)\n |-- house_id: long (nullable = true)\n |-- house_worth: double (nullable = true)\n |-- house_age: double (nullable = true)\n |-- house_size: double (nullable = true)\n```\n2. `houses_sold_data``\n```bash\n+-------+-----------------+-----------------+------------------+\n|area_id|house_purchase_id|number_of_bidders|   sold_for_amount|\n+-------+-----------------+-----------------+------------------+\n|      1|                0|                0| 70073.06059070028|\n|      1|                1|               15| 146.9198329740602|\n|      1|                2|                6|  594.802165433149|\n|      1|                3|               10| 77187.84123130841|\n|      1|                4|                1|110627.48922722359|\n+-------+-----------------+-----------------+------------------+\n\n |-- area_id: long (nullable = true)\n |-- house_purchase_id: long (nullable = true)\n |-- number_of_bidders: long (nullable = true)\n |-- sold_for_amount: double (nullable = true)\n```\n\nWe'll use this data for predicting what a house is sold for based on features about the **area** where the house is."],"metadata":{}},{"cell_type":"markdown","source":["### Generation of `houses_for_sale_data`"],"metadata":{}},{"cell_type":"code","source":["area_ids = list(range(1,51))\nhouse_sizes = []\nhouse_worths = []\nhouse_ages = []\nhouse_area_ids = []\nfor i in area_ids:\n    for j in list(range(1,100)):\n        house_sizes.append(abs(np.random.normal()*1000)/i)\n        house_worths.append(abs(np.random.normal()*10000)/i)\n        house_ages.append(abs(np.random.normal()*10000)/i)\n        house_area_ids.append(i)\nhouse_ids = list(range(len(house_area_ids)))\nhouses_for_sale_data  = pd.DataFrame({\n        'area_id':house_area_ids,\n        'house_id':house_ids,\n        'house_worth': house_worths,\n        'house_age': house_ages,\n        'house_size': house_sizes\n    })\nhouses_for_sale_data_spark_df = sqlContext.createDataFrame(houses_for_sale_data)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"code","source":["houses_for_sale_data_spark_df.show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+--------+------------------+-----------------+------------------+\narea_id|house_id|       house_worth|        house_age|        house_size|\n+-------+--------+------------------+-----------------+------------------+\n      1|       0|22839.193835817092|9517.325107143293|211.87460710460746|\n      1|       1| 6666.159123941387|9539.977492332413| 848.3002409923139|\n      1|       2| 9364.471016689939|8004.256462264917| 39.70495394765202|\n      1|       3| 3228.802593524905|6806.987729189358| 542.2954807823472|\n      1|       4|15657.544355162538|5634.052421792608|  538.806720740612|\n+-------+--------+------------------+-----------------+------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["houses_for_sale_data_spark_df.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- area_id: long (nullable = true)\n-- house_id: long (nullable = true)\n-- house_worth: double (nullable = true)\n-- house_age: double (nullable = true)\n-- house_size: double (nullable = true)\n\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["houses_for_sale_data_spark_df.write.format(\"parquet\").save(\"%s/houses_for_sale.parquet\" % MOUNT_NAME)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o401.save.\n: org.apache.spark.sql.AnalysisException: path dbfs:/mnt/demo_training_datasets/houses_for_sale.parquet already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:148)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:126)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:147)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:188)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:184)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:118)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:710)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:710)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:113)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:242)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:99)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:172)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:710)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:306)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:292)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:235)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-847864287059212&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>houses_for_sale_data_spark_df<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;parquet&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;%s/houses_for_sale.parquet&#34;</span> <span class=\"ansi-blue-fg\">%</span> MOUNT_NAME<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">save</span><span class=\"ansi-blue-fg\">(self, path, format, mode, partitionBy, **options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    737</span>             self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    738</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 739</span><span class=\"ansi-red-fg\">             </span>self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    740</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    741</span>     <span class=\"ansi-blue-fg\">@</span>since<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1.4</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#39;path dbfs:/mnt/demo_training_datasets/houses_for_sale.parquet already exists.;&#39;</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["### Generation of `houses_sold_data`"],"metadata":{}},{"cell_type":"code","source":["house_purchased_amounts = []\nhouse_purchases_bidders = []\nhouse_purchases_area_ids = []\nfor i in area_ids:\n    for j in list(range(1,1000)):\n        house_purchased_amounts.append(abs(np.random.exponential()*100000)/i)\n        house_purchases_bidders.append(int(abs(np.random.exponential()*10)/i))\n        house_purchases_area_ids.append(i)\nhouse_purchase_ids = list(range(len(house_purchases_bidders)))\nhouses_sold_data  = pd.DataFrame({\n        'area_id':house_purchases_area_ids,\n        'house_purchase_id':house_purchase_ids,\n        'number_of_bidders': house_purchases_bidders,\n        'sold_for_amount': house_purchased_amounts\n    })\nhouses_sold_data_spark_df = sqlContext.createDataFrame(houses_sold_data)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"code","source":["houses_sold_data_spark_df.show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-----------------+-----------------+------------------+\narea_id|house_purchase_id|number_of_bidders|   sold_for_amount|\n+-------+-----------------+-----------------+------------------+\n      1|                0|                4|195596.37426016538|\n      1|                1|                5| 205906.9120954603|\n      1|                2|                3| 67978.70131676484|\n      1|                3|                1|132724.12607580156|\n      1|                4|               13| 73075.43990002344|\n+-------+-----------------+-----------------+------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["houses_sold_data_spark_df.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- area_id: long (nullable = true)\n-- house_purchase_id: long (nullable = true)\n-- number_of_bidders: long (nullable = true)\n-- sold_for_amount: double (nullable = true)\n\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["houses_sold_data_spark_df.write.format(\"parquet\").save(\"%s/houses_sold.parquet\" % MOUNT_NAME)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o486.save.\n: org.apache.spark.sql.AnalysisException: path dbfs:/mnt/demo_training_datasets/houses_sold.parquet already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:148)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:126)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:147)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:188)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:184)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:118)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:710)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:710)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:113)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:242)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:99)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:172)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:710)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:306)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:292)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:235)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-847864287059217&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>houses_sold_data_spark_df<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;parquet&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;%s/houses_sold.parquet&#34;</span> <span class=\"ansi-blue-fg\">%</span> MOUNT_NAME<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">save</span><span class=\"ansi-blue-fg\">(self, path, format, mode, partitionBy, **options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    737</span>             self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    738</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 739</span><span class=\"ansi-red-fg\">             </span>self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    740</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    741</span>     <span class=\"ansi-blue-fg\">@</span>since<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1.4</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#39;path dbfs:/mnt/demo_training_datasets/houses_sold.parquet already exists.;&#39;</div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["## Feature Engineering\n\nLets generate some aggregate features such as sum and averages from our datasets on S3. \n\n1. `houses_for_sale_features`:\n\n```bash\n |-- area_id: long (nullable = true)\n |-- avg_house_age: double (nullable = true)\n |-- avg_house_size: double (nullable = true)\n |-- avg_house_worth: double (nullable = true)\n |-- sum_house_age: double (nullable = true)\n |-- sum_house_size: double (nullable = true)\n |-- sum_house_worth: double (nullable = true)\n```\n\n2. `houses_sold_features`\n\n```bash\n |-- area_id: long (nullable = true)\n |-- avg_num_bidders: double (nullable = true)\n |-- avg_sold_for: double (nullable = true)\n |-- sum_number_of_bidders: long (nullable = true)\n |-- sum_sold_for_amount: double (nullable = true)\n```"],"metadata":{}},{"cell_type":"code","source":["display(dbutils.fs.ls(MOUNT_NAME))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/demo_training_datasets/houses_for_sale.parquet/</td><td>houses_for_sale.parquet/</td><td>0</td></tr><tr><td>dbfs:/mnt/demo_training_datasets/houses_sold.parquet/</td><td>houses_sold.parquet/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["### Generate Features From `houses_for_sale_data`"],"metadata":{}},{"cell_type":"code","source":["houses_for_sale_data_spark_df = spark.read.parquet(\"%s/houses_for_sale.parquet\" % MOUNT_NAME)\nsum_houses_for_sale_df = houses_for_sale_data_spark_df.groupBy(\"area_id\").sum()\ncount_houses_for_sale_df = houses_for_sale_data_spark_df.groupBy(\"area_id\").count()\nsum_count_houses_for_sale_df = sum_houses_for_sale_df.join(count_houses_for_sale_df, \"area_id\")\nsum_count_houses_for_sale_df = sum_count_houses_for_sale_df \\\n    .withColumnRenamed(\"sum(house_age)\", \"sum_house_age\") \\\n    .withColumnRenamed(\"sum(house_worth)\", \"sum_house_worth\") \\\n    .withColumnRenamed(\"sum(house_size)\", \"sum_house_size\") \\\n    .withColumnRenamed(\"count\", \"num_rows\")\ndef compute_average_features_house_for_sale(row):\n    avg_house_worth = row.sum_house_worth/float(row.num_rows)\n    avg_house_size = row.sum_house_size/float(row.num_rows)\n    avg_house_age = row.sum_house_age/float(row.num_rows)\n    return Row(\n        sum_house_worth=row.sum_house_worth, \n        sum_house_age=row.sum_house_age,\n        sum_house_size=row.sum_house_size,\n        area_id = row.area_id,\n        avg_house_worth = avg_house_worth,\n        avg_house_size = avg_house_size,\n        avg_house_age = avg_house_age\n       )\nhouses_for_sale_features_df = sum_count_houses_for_sale_df.rdd.map(\n    lambda row: compute_average_features_house_for_sale(row)\n).toDF()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":25},{"cell_type":"code","source":["houses_for_sale_features_df.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- area_id: long (nullable = true)\n-- avg_house_age: double (nullable = true)\n-- avg_house_size: double (nullable = true)\n-- avg_house_worth: double (nullable = true)\n-- sum_house_age: double (nullable = true)\n-- sum_house_size: double (nullable = true)\n-- sum_house_worth: double (nullable = true)\n\n</div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["### Generate Features from `houses_sold_data`"],"metadata":{}},{"cell_type":"code","source":["houses_sold_data_spark_df = spark.read.parquet(\"%s/houses_sold.parquet\" % MOUNT_NAME)\nsum_houses_sold_df = houses_sold_data_spark_df.groupBy(\"area_id\").sum()\ncount_houses_sold_df = houses_sold_data_spark_df.groupBy(\"area_id\").count()\nsum_count_houses_sold_df = sum_houses_sold_df.join(count_houses_sold_df, \"area_id\")\nsum_count_houses_sold_df = sum_count_houses_sold_df \\\n    .withColumnRenamed(\"sum(number_of_bidders)\", \"sum_number_of_bidders\") \\\n    .withColumnRenamed(\"sum(sold_for_amount)\", \"sum_sold_for_amount\") \\\n    .withColumnRenamed(\"count\", \"num_rows\")\ndef compute_average_features_houses_sold(row):\n    avg_num_bidders = row.sum_number_of_bidders/float(row.num_rows)\n    avg_sold_for = row.sum_sold_for_amount/float(row.num_rows)\n    return Row(\n        sum_number_of_bidders=row.sum_number_of_bidders, \n        sum_sold_for_amount=row.sum_sold_for_amount,\n        area_id = row.area_id,\n        avg_num_bidders = avg_num_bidders,\n        avg_sold_for = avg_sold_for\n       )\nhouses_sold_features_df = sum_count_houses_sold_df.rdd.map(\n    lambda row: compute_average_features_houses_sold(row)\n).toDF()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":28},{"cell_type":"code","source":["houses_sold_features_df.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- area_id: long (nullable = true)\n-- avg_num_bidders: double (nullable = true)\n-- avg_sold_for: double (nullable = true)\n-- sum_number_of_bidders: long (nullable = true)\n-- sum_sold_for_amount: double (nullable = true)\n\n</div>"]}}],"execution_count":29},{"cell_type":"markdown","source":["## Save Features to the Feature Store\n\nThe Featue store has an abstraction of a **feature group** which is a set of features that naturally belong together that typically are computed using the same feature engineering job and the same raw dataset. \n\nLets create two feature groups:\n\n1. `houses_for_sale_featuregroup`\n\n2. `houses_sold_featuregroup`"],"metadata":{}},{"cell_type":"code","source":["featurestore.create_featuregroup(\n    houses_for_sale_features_df,\n    \"houses_for_sale_featuregroup\",\n    description=\"aggregate features of houses for sale per area\",\n    descriptive_statistics=False,\n    feature_correlation=False,\n    feature_histograms=False,\n    cluster_analysis=False\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">RestAPIError</span>                              Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/hops/featurestore.py</span> in <span class=\"ansi-cyan-fg\">create_featuregroup</span><span class=\"ansi-blue-fg\">(df, featuregroup, primary_key, description, featurestore, featuregroup_version, jobs, descriptive_statistics, feature_correlation, feature_histograms, cluster_analysis, stat_columns, num_bins, corr_method, num_clusters, partition_by, online, online_types, offline)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    637</span>                                      corr_method<span class=\"ansi-blue-fg\">=</span>corr_method<span class=\"ansi-blue-fg\">,</span> num_clusters<span class=\"ansi-blue-fg\">=</span>num_clusters<span class=\"ansi-blue-fg\">,</span> partition_by<span class=\"ansi-blue-fg\">=</span>partition_by<span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-fg\">--&gt; 638</span><span class=\"ansi-red-fg\">                                      online=online, online_types=online_types, offline=offline)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    639</span>     <span class=\"ansi-red-fg\"># If it fails, update cache and retry</span>\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/hops/featurestore_impl/core.py</span> in <span class=\"ansi-cyan-fg\">_do_create_featuregroup</span><span class=\"ansi-blue-fg\">(df, featurestore_metadata, featuregroup, primary_key, description, featurestore, featuregroup_version, jobs, descriptive_statistics, feature_correlation, feature_histograms, cluster_analysis, stat_columns, num_bins, corr_method, num_clusters, partition_by, online, online_types, offline)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1851</span>                                        num_bins<span class=\"ansi-blue-fg\">,</span> num_clusters<span class=\"ansi-blue-fg\">,</span> corr_method<span class=\"ansi-blue-fg\">,</span> featuregroup_type<span class=\"ansi-blue-fg\">,</span> featuregroup_type_dto<span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-fg\">-&gt; 1852</span><span class=\"ansi-red-fg\">                                        None, None, online)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1853</span>     fs_utils<span class=\"ansi-blue-fg\">.</span>_log<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Registering feature metadata... [COMPLETE]&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/hops/featurestore_impl/rest/rest_rpc.py</span> in <span class=\"ansi-cyan-fg\">_create_featuregroup_rest</span><span class=\"ansi-blue-fg\">(featuregroup, featurestore_id, description, featuregroup_version, jobs, features_schema, feature_corr_data, featuregroup_desc_stats_data, features_histogram_data, cluster_analysis_data, feature_corr_enabled, featuregroup_desc_stats_enabled, features_histogram_enabled, cluster_analysis_enabled, stat_columns, num_bins, num_clusters, corr_method, featuregroup_type, featuregroup_dto_type, sql_query, jdbc_connector_id, online_fg)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    293</span>                            &#34;HTTP code: {}, HTTP reason: {}, error code: {}, error msg: {}, user msg: {}&#34;.format(\n<span class=\"ansi-green-fg\">--&gt; 294</span><span class=\"ansi-red-fg\">             resource_url, response.status_code, response.reason, error_code, error_msg, user_msg))\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    295</span> \n\n<span class=\"ansi-red-fg\">RestAPIError</span>: Could not create feature group (url: /hopsworks-api/api/project/119/featurestores/67/featuregroups), server response: \n HTTP code: 400, HTTP reason: Bad Request, error code: 270089, error msg: The feature group you are trying to create does already exist., user msg: project: demo_featurestore_jim00000, featurestoreId: 67\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">RestAPIError</span>                              Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-847864287059227&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span>     feature_correlation<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">False</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span>     feature_histograms<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">False</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-fg\">----&gt; 8</span><span class=\"ansi-red-fg\">     </span>cluster_analysis<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">False</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      9</span> )\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/hops/featurestore.py</span> in <span class=\"ansi-cyan-fg\">create_featuregroup</span><span class=\"ansi-blue-fg\">(df, featuregroup, primary_key, description, featurestore, featuregroup_version, jobs, descriptive_statistics, feature_correlation, feature_histograms, cluster_analysis, stat_columns, num_bins, corr_method, num_clusters, partition_by, online, online_types, offline)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    646</span>                                      cluster_analysis<span class=\"ansi-blue-fg\">=</span>cluster_analysis<span class=\"ansi-blue-fg\">,</span> stat_columns<span class=\"ansi-blue-fg\">=</span>stat_columns<span class=\"ansi-blue-fg\">,</span> num_bins<span class=\"ansi-blue-fg\">=</span>num_bins<span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    647</span>                                      corr_method<span class=\"ansi-blue-fg\">=</span>corr_method<span class=\"ansi-blue-fg\">,</span> num_clusters<span class=\"ansi-blue-fg\">=</span>num_clusters<span class=\"ansi-blue-fg\">,</span> partition_by<span class=\"ansi-blue-fg\">=</span>partition_by<span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-fg\">--&gt; 648</span><span class=\"ansi-red-fg\">                                      online=online, online_types=online_types, offline=offline)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    649</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    650</span>     <span class=\"ansi-red-fg\"># update metadata cache since we created a new feature group and added new metadata</span>\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/hops/featurestore_impl/core.py</span> in <span class=\"ansi-cyan-fg\">_do_create_featuregroup</span><span class=\"ansi-blue-fg\">(df, featurestore_metadata, featuregroup, primary_key, description, featurestore, featuregroup_version, jobs, descriptive_statistics, feature_correlation, feature_histograms, cluster_analysis, stat_columns, num_bins, corr_method, num_clusters, partition_by, online, online_types, offline)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1850</span>                                        descriptive_statistics<span class=\"ansi-blue-fg\">,</span> feature_histograms<span class=\"ansi-blue-fg\">,</span> cluster_analysis<span class=\"ansi-blue-fg\">,</span> stat_columns<span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1851</span>                                        num_bins<span class=\"ansi-blue-fg\">,</span> num_clusters<span class=\"ansi-blue-fg\">,</span> corr_method<span class=\"ansi-blue-fg\">,</span> featuregroup_type<span class=\"ansi-blue-fg\">,</span> featuregroup_type_dto<span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-fg\">-&gt; 1852</span><span class=\"ansi-red-fg\">                                        None, None, online)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1853</span>     fs_utils<span class=\"ansi-blue-fg\">.</span>_log<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Registering feature metadata... [COMPLETE]&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1854</span>     <span class=\"ansi-green-fg\">if</span> offline<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/hops/featurestore_impl/rest/rest_rpc.py</span> in <span class=\"ansi-cyan-fg\">_create_featuregroup_rest</span><span class=\"ansi-blue-fg\">(featuregroup, featurestore_id, description, featuregroup_version, jobs, features_schema, feature_corr_data, featuregroup_desc_stats_data, features_histogram_data, cluster_analysis_data, feature_corr_enabled, featuregroup_desc_stats_enabled, features_histogram_enabled, cluster_analysis_enabled, stat_columns, num_bins, num_clusters, corr_method, featuregroup_type, featuregroup_dto_type, sql_query, jdbc_connector_id, online_fg)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    292</span>         raise RestAPIError(&#34;Could not create feature group (url: {}), server response: \\n &#34; \\\n<span class=\"ansi-green-intense-fg ansi-bold\">    293</span>                            &#34;HTTP code: {}, HTTP reason: {}, error code: {}, error msg: {}, user msg: {}&#34;.format(\n<span class=\"ansi-green-fg\">--&gt; 294</span><span class=\"ansi-red-fg\">             resource_url, response.status_code, response.reason, error_code, error_msg, user_msg))\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    295</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    296</span>     <span class=\"ansi-green-fg\">return</span> response_object\n\n<span class=\"ansi-red-fg\">RestAPIError</span>: Could not create feature group (url: /hopsworks-api/api/project/119/featurestores/67/featuregroups), server response: \n HTTP code: 400, HTTP reason: Bad Request, error code: 270089, error msg: The feature group you are trying to create does already exist., user msg: project: demo_featurestore_jim00000, featurestoreId: 67</div>"]}}],"execution_count":31},{"cell_type":"code","source":["featurestore.create_featuregroup(\n    houses_sold_features_df,\n    \"houses_sold_featuregroup\",\n    description=\"aggregate features of sold houses per area\",\n    descriptive_statistics=False,\n    feature_correlation=False,\n    feature_histograms=False,\n    cluster_analysis=False\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Registering feature metadata...\nRegistering feature metadata... [COMPLETE]\nWriting feature data to offline feature group (Hive)...\nRunning sql: use demo_featurestore_jim00000_featurestore against offline feature store\nWriting feature data to offline feature group (Hive)... [COMPLETE]\nFeature group created successfully\n</div>"]}}],"execution_count":32},{"cell_type":"markdown","source":["## Create a Training Dataset\n\nThe feature store has an abstraction of a **training dataset**, which is a dataset with a set of features (potentially from many different feature groups) and labels (in case of supervised learning). \n\nLet's create a training dataset called *predict_house_sold_for_dataset* using the following features:\n\n- avg_house_age\n- avg_house_size\n- avg_house_worth\n- avg_num_bidders\n\nand the target variable is:\n\n- avg_sold_for"],"metadata":{}},{"cell_type":"code","source":["features_df = featurestore.get_features([\"avg_house_age\", \"avg_house_size\", \n                                         \"avg_house_worth\", \"avg_num_bidders\", \n                                         \"avg_sold_for\"])\n\nfeaturestore.create_training_dataset(\n    features_df, \"predict_house_sold_for_dataset_two\",\n    data_format=\"tfrecords\",\n    descriptive_statistics=False,\n    feature_correlation=False,\n    feature_histograms=False,\n    cluster_analysis=False,\n    sink='jim-oregan' # See https://hopsworks.readthedocs.io/en/latest/featurestore/guides/featurestore.html#configuring-storage-connectors-for-the-feature-store\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Running sql: use demo_featurestore_jim00000_featurestore against offline feature store\nLogical query plan for getting 5 features from the featurestore created successfully\nSQL string for the query created successfully\nRunning sql: SELECT avg_sold_for, avg_house_age, avg_house_size, avg_num_bidders, avg_house_worth FROM houses_sold_featuregroup_1 JOIN houses_for_sale_featuregroup_1 ON houses_sold_featuregroup_1.`area_id`=houses_for_sale_featuregroup_1.`area_id` against offline feature store\nwrite feature frame, write_mode: overwrite\nTraining Dataset created successfully\n</div>"]}}],"execution_count":34},{"cell_type":"code","source":["features_df.show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------+------------------+------------------+--------------------+------------------+\n      avg_sold_for|     avg_house_age|    avg_house_size|     avg_num_bidders|   avg_house_worth|\n+------------------+------------------+------------------+--------------------+------------------+\n 3250.164812645495| 261.2603487984262|23.953524668767752|0.043043043043043044|259.63553145772033|\n 2377.364099766353|184.93581102333127|16.744133746387067|0.014014014014014014| 188.3377117070782|\n 3283.470920463867|245.18125243592596|28.894407116303206| 0.05305305305305305| 286.5947879163963|\n2463.8355210458567|167.80308871239976| 18.31532898187385|0.018018018018018018|191.01302219730556|\n 2123.221433295513|172.61531843664673|15.745658708242688|0.011011011011011011|164.82636105543025|\n+------------------+------------------+------------------+--------------------+------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":35},{"cell_type":"code","source":["features_df = featurestore.get_features(\n    [\"team_budget\", \"average_attendance\", \"average_player_age\",\n    \"team_position\", \"sum_attendance\", \n     \"average_player_rating\", \"average_player_worth\", \"sum_player_age\",\n     \"sum_player_rating\", \"sum_player_worth\", \"sum_position\", \n     \"average_position\"\n    ]\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Running sql: use demo_featurestore_jim00000_featurestore against offline feature store\nLogical query plan for getting 12 features from the featurestore created successfully\nSQL string for the query created successfully\nRunning sql: SELECT team_budget, average_position, sum_player_rating, average_attendance, average_player_worth, sum_player_worth, sum_position, sum_attendance, average_player_rating, team_position, sum_player_age, average_player_age FROM teams_features_1 JOIN season_scores_features_1 JOIN players_features_1 JOIN attendances_features_1 ON teams_features_1.`team_id`=season_scores_features_1.`team_id` AND teams_features_1.`team_id`=players_features_1.`team_id` AND teams_features_1.`team_id`=attendances_features_1.`team_id` against offline feature store\n</div>"]}}],"execution_count":36},{"cell_type":"code","source":["display(dbutils.fs.ls(MOUNT_NAME))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/demo_training_datasets/TRAINING_DATASETS/</td><td>TRAINING_DATASETS/</td><td>0</td></tr><tr><td>dbfs:/mnt/demo_training_datasets/houses_sold.parquet/</td><td>houses_sold.parquet/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":37},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":38}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"","name":"pysparkkernel"},"language_info":{"codemirror_mode":{"name":"python","version":2},"mimetype":"text/x-python","name":"pyspark","pygments_lexer":"ipython3"},"name":"FeatureStoreQuickStart","notebookId":847864287059197},"nbformat":4,"nbformat_minor":0}
