{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "LsYrLKRGOebx",
    "outputId": "394665b0-5047-4a57-8355-454e79885cc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>42</td><td>application_1587025294218_0040</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://gpu.c.dpe-cloud-mle.internal:8088/proxy/application_1587025294218_0040/\">Link</a></td><td><a target=\"_blank\" href=\"http://gpu.c.dpe-cloud-mle.internal:8042/node/containerlogs/container_e15_1587025294218_0040_01_000001/demo_deep_learning_admin000__meb10000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "2.2.0-rc2\n",
      "Num GPUs Available:  0\n",
      "XLA_FLAGS='None'"
     ]
    }
   ],
   "source": [
    "import contextlib\n",
    "import functools\n",
    "import gc\n",
    "import multiprocessing\n",
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "import time\n",
    "import timeit\n",
    "import urllib.request\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "with tf.device(\"GPU:0\"):\n",
    "  tf.ones(())  # Make sure we can run on GPU\n",
    "\n",
    "#data_root = \"/tmp/demo_images\"\n",
    "#profile_dir = os.path.join(data_root, \"profiles\")\n",
    "#os.makedirs(profile_dir, exist_ok=True)\n",
    "\n",
    "# This ensures that XLA and ptxas work well together, and helps with scaling.\n",
    "print(\"XLA_FLAGS='{}'\".format(os.getenv(\"XLA_FLAGS\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YlbpK_YudZBO"
   },
   "source": [
    "## Configure task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TV3APg_xdWdm"
   },
   "outputs": [],
   "source": [
    "num_images_per_label = 50000\n",
    "RESOLUTION = (224, 224)\n",
    "NUM_CHANNELS = 3\n",
    "NUM_TOTAL_IMAGES = num_images_per_label * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "81WaEDOSPSfC"
   },
   "source": [
    "## Model and data setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ugyuOzS4O_Tn"
   },
   "outputs": [],
   "source": [
    "#def get_input_shape():\n",
    "#  if tf.keras.backend.image_data_format() == \"channels_last\":\n",
    "#    return RESOLUTION + (NUM_CHANNELS,)\n",
    "#  elif tf.keras.backend.image_data_format() == \"channels_first\":\n",
    "#    return (NUM_CHANNELS,) + RESOLUTION\n",
    "#  raise ValueError(\"Unknown format.\")\n",
    "#\n",
    "## Native jpg layout.\n",
    "#NHWC_INPUT_SHAPE = RESOLUTION + (NUM_CHANNELS,)\n",
    "\n",
    "def get_input_shape():\n",
    "    # Input image dimensions\n",
    "    img_rows, img_cols = 28, 28\n",
    "    return (img_rows, img_cols, 1)\n",
    "\n",
    "\n",
    "def transform_image(image):\n",
    "  image = image / 255.0  # Scale occurs in random transformation\n",
    "  \n",
    "  image = tf.image.random_flip_left_right(image)\n",
    "  image = tf.image.random_flip_up_down(image)\n",
    "  image += tf.random.normal(tf.shape(image), mean=-0.5, stddev=0.1, dtype=image.dtype)\n",
    "  return image\n",
    "\n",
    "\n",
    "def make_model(input_dtype=tf.float32, transform_on_device=False):\n",
    "    \n",
    "  kernel = 3\n",
    "  pool = 2\n",
    "  dropout = 0.5\n",
    "  num_classes = 10\n",
    "\n",
    "  input_shape = get_input_shape()\n",
    "\n",
    "  input_layer = tf.keras.layers.Conv2D(32, kernel_size=(kernel, kernel),\n",
    "                                       padding='same',\n",
    "                                       activation='relu',\n",
    "                                       input_shape=input_shape,\n",
    "                                       dtype=input_dtype\n",
    "                                      )\n",
    "\n",
    "  layer = input_layer\n",
    "  if transform_on_device:\n",
    "    layer = tf.keras.layers.Lambda(transform_image)(layer)\n",
    "    \n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(layer)\n",
    "  model.add(tf.keras.layers.Conv2D(64, (kernel, kernel),  padding='same',activation='relu'))\n",
    "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(pool, pool)))\n",
    "  model.add(tf.keras.layers.Dropout(dropout))\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "  model.add(tf.keras.layers.Dense(1024, activation='relu'))        \n",
    "  model.add(tf.keras.layers.Dropout(dropout))\n",
    "  model.add(tf.keras.layers.Dense(num_classes))\n",
    "\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GlB2GqBKvWJH"
   },
   "source": [
    "## Training loop\n",
    "\n",
    "But wait, this is much more complicated than the slides...\n",
    "\n",
    "Indeed. This is because it runs all of the options discussed, and tries to clean up after itsef (Hence the context managers), profiles, and provide accurate steady state measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FIQenvZd-q93"
   },
   "source": [
    "### Toggles for various optimizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CUacwlUg-1pe"
   },
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def stop_profiler():\n",
    "  \"\"\"Used to guarantee that the TensorFlow profiler does not remain on.\n",
    "  \n",
    "  We don't want to mix traces from different runs as it would make them hard\n",
    "  to interpret, so this ensures that the profiler is disabled even if our\n",
    "  training loop crashes.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    yield\n",
    "  finally:\n",
    "    tf.summary.trace_off()\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def use_mixed_precision(loss_scale):\n",
    "  \"\"\"Enable mixed precision, and reset the policy after training.\"\"\"\n",
    "  old_policy = tf.keras.mixed_precision.experimental.global_policy()\n",
    "\n",
    "  try:\n",
    "    policy = tf.compat.v2.keras.mixed_precision.experimental.Policy(\n",
    "        \"mixed_float16\", loss_scale=loss_scale)\n",
    "    tf.keras.mixed_precision.experimental.set_policy(policy)\n",
    "    yield\n",
    "  finally:\n",
    "    tf.keras.mixed_precision.experimental.set_policy(old_policy)\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def enable_xla():\n",
    "  \"\"\"Enable XLA, and disable it after training is complete.\"\"\"\n",
    "  try:\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    yield\n",
    "  finally:\n",
    "    tf.config.optimizer.set_jit(False)\n",
    "\n",
    "\n",
    "_THREADS_PER_GPU = 2\n",
    "@contextlib.contextmanager\n",
    "def tuning_context():\n",
    "  \"\"\"Hand tuned model configurations.\n",
    "  \n",
    "  Historically these knobs have improved performance, but as of 10/28/2019 they\n",
    "  actually hurt performance. However they are provided simply for completeness\n",
    "  to show some of the lower level knobs.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    os.environ['TF_GPU_THREAD_MODE'] = \"gpu_private\"\n",
    "    os.environ['TF_GPU_THREAD_COUNT'] = str(_THREADS_PER_GPU)\n",
    "    tf.keras.backend.set_image_data_format(\"channels_first\")\n",
    "    yield\n",
    "  finally:\n",
    "    os.environ.pop('TF_GPU_THREAD_MODE', None)\n",
    "    os.environ.pop('TF_GPU_THREAD_COUNT', None)\n",
    "    tf.keras.backend.set_image_data_format(\"channels_last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tq14j9KkqG8z"
   },
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def null_context():\n",
    "  \"\"\"Implementation detail. Used if a given toggle is disabled.\"\"\"\n",
    "  yield\n",
    "\n",
    "def null_decorator(f):\n",
    "  \"\"\"Implementation detail. Used if tf.function is disabled.\"\"\"\n",
    "  return f\n",
    "\n",
    "def train_model(data_fn, global_batch_size, use_tf_function=False, \n",
    "                strategy=None, xla=False, mixed_precision=False, \n",
    "                loss_scale=\"dynamic\", collect_profile=False, tuned=False, \n",
    "                transform_on_device=False):\n",
    "\n",
    "  # Ensure runs are independent.\n",
    "  tf.keras.backend.clear_session()\n",
    "  gc.collect()\n",
    "  time.sleep(3)\n",
    "\n",
    "  if tuned:\n",
    "    assert strategy, \"Tuned version assumes a distribuation strategy is present.\"\n",
    "\n",
    "  dtype = tf.float16 if mixed_precision else tf.float32\n",
    "  step_decorator = tf.function if use_tf_function and not strategy else null_decorator\n",
    "  strategy_scope = strategy.scope() if strategy else null_context()\n",
    "  xla_scope = enable_xla() if xla else null_context()\n",
    "  precision_scope = use_mixed_precision(loss_scale) if mixed_precision else null_context()\n",
    "  tuning_scope = tuning_context() if tuned else null_context()\n",
    "\n",
    "  with strategy_scope, xla_scope, precision_scope, stop_profiler(), tuning_scope:\n",
    "    model = make_model(input_dtype=dtype, transform_on_device=transform_on_device)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "    if mixed_precision:\n",
    "      optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, loss_scale=loss_scale)\n",
    "\n",
    "#    model.compile(\n",
    "#            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#            optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "#            metrics=['accuracy'],\n",
    "#        )\n",
    "    \n",
    "    @step_decorator\n",
    "    def replica_step(features, labels):\n",
    "      with tf.GradientTape() as tape:\n",
    "        logits = model(features, training=True)\n",
    "#        replica_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels, logits)\n",
    "        replica_loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True, axis=-1)\n",
    "        loss = tf.nn.compute_average_loss(replica_loss, global_batch_size=global_batch_size)\n",
    "      grads = tape.gradient(loss, model.trainable_variables)\n",
    "      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "      return loss\n",
    "\n",
    "    step_fn = replica_step\n",
    "    if strategy and use_tf_function:\n",
    "      @tf.function\n",
    "      def replicated_step(features, labels):\n",
    "        loss = strategy.experimental_run_v2(replica_step, (features, labels))\n",
    "        return loss\n",
    "\n",
    "      step_fn = replicated_step\n",
    "\n",
    "    if strategy:\n",
    "      dataset = data_fn(batch_size=global_batch_size, dtype=dtype, \n",
    "                        transform_on_device=transform_on_device)\n",
    "      if tuned:\n",
    "        options = tf.data.Options()\n",
    "        private_threads = (multiprocessing.cpu_count() - \n",
    "                           strategy.num_replicas_in_sync * (_THREADS_PER_GPU + 1))\n",
    "        options.experimental_threading.private_threadpool_size = private_threads\n",
    "        dataset = dataset.with_options(options)\n",
    "      data = strategy.experimental_distribute_dataset(dataset)\n",
    "    else:\n",
    "      assert not transform_on_device\n",
    "      data = data_fn(batch_size=global_batch_size, dtype=dtype)\n",
    "\n",
    "    schedule = [\n",
    "        5,                             # Burn in\n",
    "        5 if collect_profile else 0,   # Profiling\n",
    "        30,                            # Steady state throughput\n",
    "    ]\n",
    "    times = []\n",
    "    \n",
    "    for step_number, inputs in enumerate(data):\n",
    "      loss = step_fn(*inputs)\n",
    "\n",
    "      # Burn in\n",
    "      if schedule[0]:\n",
    "        schedule[0] -= 1\n",
    "        if not schedule[0]:\n",
    "          print(\"Burn in complete.\")\n",
    "          if schedule[1]:\n",
    "            time.sleep(2)  # Let running ops finish to start from a clean trace.\n",
    "            tf.summary.trace_on(profiler=True)\n",
    "          else:\n",
    "            # Skip straight to steady state throughput\n",
    "            start_time = timeit.default_timer()\n",
    "            iter_count = 0\n",
    "        continue\n",
    "\n",
    "      # Op profiler\n",
    "      if schedule[1]:\n",
    "        schedule[1] -= 1\n",
    "        if not schedule[1]:\n",
    "          tf.summary.trace_export(name=\"my_trace\", profiler_outdir=profile_dir)\n",
    "          start_time = timeit.default_timer()\n",
    "          iter_count = 0\n",
    "        continue\n",
    "\n",
    "      # Profile steady state execution\n",
    "      schedule[2] -= 1\n",
    "      iter_count += 1\n",
    "      times.append(timeit.default_timer())\n",
    "      if not schedule[2]:\n",
    "        break\n",
    "        \n",
    "    run_time = timeit.default_timer() - start_time\n",
    "    step_time = run_time / iter_count\n",
    "    # print(np.array(times[1:]) - np.array(times[:-1]))\n",
    "    data = {}\n",
    "    data[global_batch_size] = {\"batch_size\": global_batch_size, \"steps\": iter_count,\"Mean_step_time_sec\": step_time, \"Images_per_sec\": int(global_batch_size / step_time)}\n",
    "    json_data = json.dumps(data)\n",
    "    with open(\"stats.json\", \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "        \n",
    "    print(\"{} steps\".format(iter_count))\n",
    "    print(\"Mean step time: {:>6.2f} sec\".format(step_time))\n",
    "    print(\"Images / sec:   {:>6d}\".format(int(global_batch_size / step_time)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YAYjN5yoNF9h"
   },
   "source": [
    "## First pass at a training function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uzeWWSDYg084"
   },
   "source": [
    "### Define a generator based data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eClYVDBxPOR8"
   },
   "outputs": [],
   "source": [
    "def random_flip(image):\n",
    "  hflip = np.random.random() > 0.5\n",
    "  vflip = np.random.random() > 0.5\n",
    "  if hflip and vflip:\n",
    "    image = cv2.flip(image, -1)\n",
    "  elif hflip:\n",
    "    image = cv2.flip(image, -1)\n",
    "  elif vflip:\n",
    "    image = cv2.flip(image, 1)\n",
    "  return image\n",
    "\n",
    "def normalize_and_add_noise(image):\n",
    "  image = image.astype(np.float32) / 255 - 0.5\n",
    "  image += np.random.normal(loc=0, scale=0.1, size=image.shape)\n",
    "  return image\n",
    "\n",
    "def make_batch(features, labels):\n",
    "  x = tf.convert_to_tensor(np.stack(features, axis=0))\n",
    "  y = tf.convert_to_tensor(np.array(labels, dtype=np.float32)[:, np.newaxis])\n",
    "  features.clear()\n",
    "  labels.clear()\n",
    "  return x, y\n",
    "\n",
    "def data_generator(batch_size, **kwargs):\n",
    "  epoch_order = np.random.permutation(get_paths_and_labels())\n",
    "  features, labels = [], []\n",
    "  for image_path, label in epoch_order:\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Resize to training resolution\n",
    "    image = cv2.resize(image, RESOLUTION)\n",
    "\n",
    "    # Randomly horizontal and vertical flip\n",
    "    image = random_flip(image)\n",
    "\n",
    "    # Normalize, center, and add Gaussian noise\n",
    "    image = normalize_and_add_noise(image)\n",
    "    \n",
    "    features.append(image)\n",
    "    labels.append(label)\n",
    "    if len(features) == batch_size:\n",
    "      yield make_batch(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "cnEMGFP1egej",
    "outputId": "a651a152-1d3b-41ad-f47d-fa58740cc988"
   },
   "outputs": [],
   "source": [
    "#for batch_size in [32, 64, 128]:\n",
    "#  print(\"Batch size: {}\".format(batch_size))\n",
    "#  train_model(data_generator, batch_size)\n",
    "#  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QNqkLxasBDY0"
   },
   "source": [
    "## Add tf.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q3pznH5zSHxq"
   },
   "source": [
    "### Use the images directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jvrj6QNYSOaU"
   },
   "outputs": [],
   "source": [
    "#def make_jpg_dataset(batch_size, dtype=tf.float32, transform_on_device=False, \n",
    "#                     already_resized=False):\n",
    "#  if already_resized:\n",
    "#    raise NotImplementedError(\n",
    "#        \"`already_resized` is only implemented for the TFRecords path.\")\n",
    "#\n",
    "#  def get_bytes_and_label(filepath):\n",
    "#    image_bytes = tf.io.read_file(filepath)\n",
    "#    label = tf.strings.regex_full_match(filepath, pos_dir + \".+\")\n",
    "#    return image_bytes, tf.expand_dims(label, 0)\n",
    "#\n",
    "#  def process_image(image_bytes, label):\n",
    "#    image = tf.io.decode_jpeg(image_bytes)\n",
    "#    image = tf.cast(image, dtype)\n",
    "#    image = tf.image.resize(image, RESOLUTION)\n",
    "#\n",
    "#    if tf.shape(image)[2] == 1:\n",
    "#      # Some images are greyscale.\n",
    "#      image = tf.tile(image, (1, 1, 3))\n",
    "#\n",
    "#    image.set_shape(NHWC_INPUT_SHAPE)\n",
    "#\n",
    "#    if not transform_on_device:\n",
    "#      image = transform_image(image)\n",
    "#\n",
    "#    if tf.keras.backend.image_data_format() == \"channels_first\":\n",
    "#      image = tf.transpose(image, (2, 0, 1))\n",
    "#    \n",
    "#    return image, tf.cast(label, dtype)\n",
    "#\n",
    "#  dataset = tf.data.Dataset.list_files([pos_dir + \"/*\", neg_dir + \"/*\"])\n",
    "#  dataset = dataset.shuffle(NUM_TOTAL_IMAGES)\n",
    "#  dataset = dataset.map(get_bytes_and_label, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#  dataset = dataset.map(process_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#  dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "#  return dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A9Frb0qbSLrG"
   },
   "source": [
    "### Use TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bZQvIamLw87p"
   },
   "outputs": [],
   "source": [
    "def make_dataset(batch_size, dtype=tf.float32, transform_on_device=False, \n",
    "                 already_resized=True):\n",
    "\n",
    "  # put already_resized false if you would like to measure reading jpg files directly\n",
    "\n",
    "#  def parse_fn(record):\n",
    "#    RECORD_SCHEMA = {\n",
    "#     \"image\": tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "#     \"label\": tf.io.FixedLenFeature([1], dtype=tf.int64)\n",
    "#    }\n",
    "#    record = tf.io.parse_single_example(record, RECORD_SCHEMA)\n",
    "#    image = tf.io.decode_jpeg(record[\"image\"])\n",
    "#    image = tf.cast(image, dtype)\n",
    "#    if not already_resized:\n",
    "#      image = tf.image.resize(image, RESOLUTION)\n",
    "#\n",
    "#    if tf.shape(image)[2] == 1:\n",
    "#      # Some images are greyscale.\n",
    "#      image = tf.tile(image, (1, 1, 3))\n",
    "#\n",
    "#    image.set_shape(NHWC_INPUT_SHAPE)\n",
    "#    \n",
    "#    if not transform_on_device:\n",
    "#      image = transform_image(image)\n",
    "#\n",
    "#    if tf.keras.backend.image_data_format() == \"channels_first\":\n",
    "#      image = tf.transpose(image, (2, 0, 1))\n",
    "#    \n",
    "#    return image, tf.cast(record[\"label\"], dtype)\n",
    "\n",
    "    \n",
    "  def parse_fn(record):\n",
    "    img_rows, img_cols = 28, 28\n",
    "    RECORD_SCHEMA = {\n",
    "        'image_raw': tf.io.FixedLenFeature([img_rows * img_cols], tf.float32),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    record = tf.io.parse_single_example(record, RECORD_SCHEMA)\n",
    "    image = record['image_raw']\n",
    "    label = record['label']   \n",
    "    return image, label\n",
    "\n",
    "  def _reshape_img(image, label):\n",
    "      image = tf.reshape(image, [28, 28, 1])\n",
    "      # label = tf.one_hot(label, num_classes)\n",
    "      return image, label\n",
    "\n",
    "\n",
    "  from hops import hdfs\n",
    "  import pydoop.hdfs as pydoop\n",
    "\n",
    "#  data_dir = hdfs.project_path()\n",
    "#  filenames = pydoop.path.abspath(data_dir + \"TourData/tfrecord_data/\")\n",
    "#  pattern = tf.io.gfile.glob(filenames + \"*.tfrecords\")\n",
    "\n",
    "  data_dir = hdfs.project_path()\n",
    "  filenames = pydoop.path.abspath(data_dir + \"TourData/mnist/train/df-mnist_train.tfrecord\")\n",
    "  pattern = tf.io.gfile.glob(filenames + \"/part-r-*\")\n",
    "\n",
    "  dataset = tf.data.Dataset.list_files(pattern)\n",
    "  dataset = dataset.interleave(tf.data.TFRecordDataset, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  dataset = dataset.shuffle(4 * batch_size)\n",
    "  dataset = dataset.map(parse_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  dataset = dataset.map(\n",
    "        _reshape_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "  return dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ttFWvxziZap4"
   },
   "source": [
    "### *already_resized* and *transform_on_device*\n",
    "\n",
    "Even with maximum parallelization, the CPU can only produce a bit over 3000 examples per second. This is fine for 1 GPU training since the GPU maxes out in the low 2000's, but would prevent reasonable scaling to more GPUs. This is due to two principle bottlenecks:\n",
    "\n",
    "#### Native image size\n",
    "\n",
    "The downloaded thumbnails tend to be around 400x600 resolution, whereas we're training at 224x224. This means that we have to move approximately 6x as many bytes into memory, spend a correspondingly long time decoding the jpg's, and incur an extra memcpy for the resize. It turns out to be quite important to resize the images to 224x224 and use those resized images in the input pipeline.\n",
    "\n",
    "#### Random augmentation\n",
    "\n",
    "The CPU simply cannot add noise to the images quickly enough to keep up with the GPU, so to maintain performance for the multi-GPU case we have to move those transformations from the input pipeline to the start of the model. Even though that puts them on the critical path, the GPU can process the augmentation so quickly that it isn't an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "O8w1vYVjwQkG",
    "outputId": "89839bbf-b4e6-41b7-bf9d-536a6181cc43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0158557931850241 4.883587235934101 308 312 (32, 28, 28, 1)\n",
      "Use TFRecords                                    2000 Images / sec\n",
      "0.012512271983818114 3.853782159043476 308 312 (32, 28, 28, 1)\n",
      "Use TFRecords with scaling optimizations         2500 Images / sec\n",
      "0.00542479583306327 16.93079052399844 3121 3125 (32, 28, 28, 1)\n",
      "Synthetic data.                                  5800 Images / sec"
     ]
    }
   ],
   "source": [
    "def measure_dataset_throughput(dataset_fn, label=\"\"):\n",
    "  count = 0\n",
    "  bluh = 0\n",
    "  batch_size = 32\n",
    "#  for i, _ in enumerate(dataset_fn(batch_size=batch_size).take(50)):\n",
    "  for i, _ in enumerate(dataset_fn(batch_size=batch_size)):\n",
    "    bluh += 1    \n",
    "    if i == 3:\n",
    "      st = timeit.default_timer()\n",
    "    if i > 3:\n",
    "      count += 1\n",
    "  step_time = (timeit.default_timer() - st) / count\n",
    "  print(step_time, (timeit.default_timer() - st), count, bluh, _[0].shape)  \n",
    "  print(\"{:<45}  {:>6.0f} Images / sec\".format(label, batch_size / step_time // 100 * 100))\n",
    "\n",
    "\n",
    "def make_synthetic_dataset(batch_size, dtype=tf.float32, **kwargs):\n",
    "  num_images_per_label  = 50000\n",
    "  dataset = tf.data.Dataset.range(2 * num_images_per_label)\n",
    "  def map_fn(_):\n",
    "    x = tf.zeros(shape=get_input_shape(), dtype=dtype)\n",
    "    y = tf.zeros(shape=(1,), dtype=dtype)\n",
    "    return x, y\n",
    "  dataset = dataset.map(map_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "  return dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "#measure_dataset_throughput(\n",
    "#    functools.partial(make_jpg_dataset, dtype=tf.float16),\n",
    "#    \"Use JPEGs directly\")\n",
    "\n",
    "measure_dataset_throughput(\n",
    "    functools.partial(make_dataset, dtype=tf.float16),\n",
    "    \"Use TFRecords\")\n",
    "\n",
    "measure_dataset_throughput(\n",
    "    functools.partial(make_dataset, dtype=tf.float16, transform_on_device=True, \n",
    "                      already_resized=True),\n",
    "    \"Use TFRecords with scaling optimizations\")\n",
    "\n",
    "# Use synthetic data to ensure model is not input bound.\n",
    "measure_dataset_throughput(\n",
    "    functools.partial(make_synthetic_dataset, dtype=tf.float16),\n",
    "    \"Synthetic data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "6PzCrL832yhu",
    "outputId": "8d0335c0-7e21-418c-83d8-a495c9824bdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment \n",
      "\n",
      "('hdfs://10.128.0.3:8020/Projects/demo_deep_learning_admin000/Experiments/application_1587025294218_0040_1', {'log': 'Experiments/application_1587025294218_0040_1/output.log'})"
     ]
    }
   ],
   "source": [
    "def debugger_example():\n",
    "    from hops import hdfs\n",
    "    from hops import tensorboard    \n",
    "    # How you easily get the TensorBoard logdir for summaries\n",
    "    tensorboard_logdir = tensorboard.logdir()\n",
    "    \n",
    "    for batch_size in [32, 64, 128]:\n",
    "      print(\"Batch size: {}\".format(batch_size))\n",
    "      train_model(make_dataset, batch_size)\n",
    "      hdfs.copy_to_hdfs(\"stats.json\", \"Logs/tf_performance/no_extra_opt-{}.json\".format(batch_size), overwrite=True)  \n",
    "      print()\n",
    "    \n",
    "\n",
    "from hops import experiment\n",
    "experiment.launch(debugger_example, name='tensorboard debugger', local_logdir=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UVJnWI_OBJid"
   },
   "source": [
    "## Add tf.function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "01mZRP8p9jny",
    "outputId": "4c926ddc-12a0-45f2-e487-c62efdbf8c48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment \n",
      "\n",
      "('hdfs://10.128.0.3:8020/Projects/demo_deep_learning_admin000/Experiments/application_1587025294218_0040_2', {'log': 'Experiments/application_1587025294218_0040_2/output.log'})"
     ]
    }
   ],
   "source": [
    "def debugger_example():\n",
    "\n",
    "    from hops import hdfs\n",
    "    from hops import tensorboard    \n",
    "    # How you easily get the TensorBoard logdir for summaries\n",
    "    tensorboard_logdir = tensorboard.logdir()\n",
    "    \n",
    "    for batch_size in [32, 64, 128]:\n",
    "      print(\"Batch size: {}\".format(batch_size))\n",
    "      train_model(make_dataset, batch_size, use_tf_function=True)\n",
    "      hdfs.copy_to_hdfs(\"stats.json\", \"Logs/tf_performance/tf_function_{}.json\".format(batch_size), overwrite=True)  \n",
    "      print()\n",
    "\n",
    "from hops import experiment\n",
    "experiment.launch(debugger_example, name='tensorboard debugger', local_logdir=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7rDLkJGIBNtU"
   },
   "source": [
    "## Add XLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "pKKGxkTxasM8",
    "outputId": "fccf6aa2-f83c-4a73-c802-295cc6b273cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment \n",
      "\n",
      "('hdfs://10.128.0.3:8020/Projects/demo_deep_learning_admin000/Experiments/application_1587025294218_0040_3', {'log': 'Experiments/application_1587025294218_0040_3/output.log'})"
     ]
    }
   ],
   "source": [
    "def debugger_example():\n",
    "\n",
    "    from hops import hdfs\n",
    "    from hops import tensorboard    \n",
    "    # How you easily get the TensorBoard logdir for summaries\n",
    "    tensorboard_logdir = tensorboard.logdir()\n",
    "    \n",
    "    for batch_size in [32 , 64, 128]:\n",
    "      print(\"Batch size: {}\".format(batch_size))\n",
    "      train_model(make_dataset, batch_size, use_tf_function=True, xla=True)\n",
    "      hdfs.copy_to_hdfs(\"stats.json\", \"Logs/tf_performance/tf_function_xla_{}.json\".format(batch_size), overwrite=True)  \n",
    "      print()\n",
    "\n",
    "from hops import experiment\n",
    "experiment.launch(debugger_example, name='tensorboard debugger', local_logdir=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WSAIDkr-BQid"
   },
   "source": [
    "## Add mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "colab_type": "code",
    "id": "D5CmKot0-Cfa",
    "outputId": "55c81131-515a-43e2-bc76-a7c406185680"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment \n",
      "\n",
      "('hdfs://10.128.0.3:8020/Projects/demo_deep_learning_admin000/Experiments/application_1587025294218_0040_4', {'log': 'Experiments/application_1587025294218_0040_4/output.log'})"
     ]
    }
   ],
   "source": [
    "def debugger_example():\n",
    "     \n",
    "    from hops import hdfs\n",
    "    from hops import tensorboard    \n",
    "    # How you easily get the TensorBoard logdir for summaries\n",
    "    tensorboard_logdir = tensorboard.logdir()\n",
    "    \n",
    "    \n",
    "    for batch_size in [32, 64, 128, 256]:\n",
    "      print(\"Batch size: {}\".format(batch_size))\n",
    "      train_model(make_dataset, batch_size, use_tf_function=True, xla=True, mixed_precision=True)\n",
    "      hdfs.copy_to_hdfs(\"stats.json\", \"Logs/tf_performance/tf_function_xla_mixprec_{}.json\".format(batch_size), overwrite=True)  \n",
    "      print()\n",
    "\n",
    "from hops import experiment\n",
    "experiment.launch(debugger_example, name='tensorboard debugger', local_logdir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             batch_size  steps  Mean_step_time_sec  \\\n",
      "tf_function_32                       32     30            0.023410   \n",
      "tf_function_xla_32                   32     30            0.002274   \n",
      "tf_function_xla_mixprec_32           32     30            0.027946   \n",
      "no_extra_opt                         64     30            0.060711   \n",
      "tf_function_64                       64     30            0.028070   \n",
      "tf_function_xla_64                   64     30            0.024084   \n",
      "tf_function_xla_mixprec_64           64     30            0.033215   \n",
      "tf_function_128                     128     30            0.034588   \n",
      "tf_function_xla_128                 128     30            0.055679   \n",
      "tf_function_xla_mixprec_128         128     30            0.035541   \n",
      "tf_function_xla_mixprec_256         256     30            0.123290   \n",
      "\n",
      "                             Images_per_sec  \n",
      "tf_function_32                         1366  \n",
      "tf_function_xla_32                    14069  \n",
      "tf_function_xla_mixprec_32             1145  \n",
      "no_extra_opt                           1054  \n",
      "tf_function_64                         2280  \n",
      "tf_function_xla_64                     2657  \n",
      "tf_function_xla_mixprec_64             1926  \n",
      "tf_function_128                        3700  \n",
      "tf_function_xla_128                    2298  \n",
      "tf_function_xla_mixprec_128            3601  \n",
      "tf_function_xla_mixprec_256            2076"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from hops import hdfs\n",
    "import pydoop.hdfs as pydoop\n",
    "\n",
    "import json\n",
    "from hops import hdfs\n",
    "import pydoop.hdfs as pydoop\n",
    "\n",
    "data_dir = hdfs.project_path()\n",
    "final_stats = {}\n",
    "for i in pydoop.hdfs().list_directory(data_dir + \"Logs/tf_performance/\"):\n",
    "    path_list = i['name'].split(\"/\")\n",
    "    method = path_list[len(path_list)-1].split(\".\")[0]\n",
    "    method = method.split(\"-\")[0]\n",
    "    for z in pydoop.hdfs().list_directory(i['name']):\n",
    "        stats = json.loads(hdfs.load(z['name']))\n",
    "        stats[method] = stats.pop(list(stats.keys())[0])\n",
    "        final_stats.update(stats)\n",
    "        \n",
    "#print(final_stats)        \n",
    "pd.options.display.max_columns = None\n",
    "dataframe = pd.DataFrame.from_dict(final_stats, orient=\"index\").sort_values(by=['batch_size'])        \n",
    "dataframe        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S3k44OU66QXM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tf_world.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}