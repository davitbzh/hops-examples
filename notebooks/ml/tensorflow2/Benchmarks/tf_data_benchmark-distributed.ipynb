{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "LsYrLKRGOebx",
    "outputId": "394665b0-5047-4a57-8355-454e79885cc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>43</td><td>application_1587025294218_0041</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://gpu.c.dpe-cloud-mle.internal:8088/proxy/application_1587025294218_0041/\">Link</a></td><td><a target=\"_blank\" href=\"http://gpu.c.dpe-cloud-mle.internal:8042/node/containerlogs/container_e15_1587025294218_0041_01_000001/demo_deep_learning_admin000__meb10000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "2.2.0-rc2\n",
      "Num GPUs Available:  0\n",
      "XLA_FLAGS='None'"
     ]
    }
   ],
   "source": [
    "import contextlib\n",
    "import functools\n",
    "import gc\n",
    "import multiprocessing\n",
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "import time\n",
    "import timeit\n",
    "import urllib.request\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "with tf.device(\"GPU:0\"):\n",
    "  tf.ones(())  # Make sure we can run on GPU\n",
    "\n",
    "#data_root = \"/tmp/demo_images\"\n",
    "#profile_dir = os.path.join(data_root, \"profiles\")\n",
    "#os.makedirs(profile_dir, exist_ok=True)\n",
    "\n",
    "# This ensures that XLA and ptxas work well together, and helps with scaling.\n",
    "print(\"XLA_FLAGS='{}'\".format(os.getenv(\"XLA_FLAGS\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YlbpK_YudZBO"
   },
   "source": [
    "## Configure task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TV3APg_xdWdm"
   },
   "outputs": [],
   "source": [
    "num_images_per_label = 50000\n",
    "RESOLUTION = (224, 224)\n",
    "NUM_CHANNELS = 3\n",
    "NUM_TOTAL_IMAGES = num_images_per_label * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "81WaEDOSPSfC"
   },
   "source": [
    "## Model and data setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ugyuOzS4O_Tn"
   },
   "outputs": [],
   "source": [
    "#def get_input_shape():\n",
    "#  if tf.keras.backend.image_data_format() == \"channels_last\":\n",
    "#    return RESOLUTION + (NUM_CHANNELS,)\n",
    "#  elif tf.keras.backend.image_data_format() == \"channels_first\":\n",
    "#    return (NUM_CHANNELS,) + RESOLUTION\n",
    "#  raise ValueError(\"Unknown format.\")\n",
    "#\n",
    "## Native jpg layout.\n",
    "#NHWC_INPUT_SHAPE = RESOLUTION + (NUM_CHANNELS,)\n",
    "\n",
    "def get_input_shape():\n",
    "    # Input image dimensions\n",
    "    img_rows, img_cols = 28, 28\n",
    "    return (img_rows, img_cols, 1)\n",
    "\n",
    "\n",
    "def transform_image(image):\n",
    "  image = image / 255.0  # Scale occurs in random transformation\n",
    "  \n",
    "  image = tf.image.random_flip_left_right(image)\n",
    "  image = tf.image.random_flip_up_down(image)\n",
    "  image += tf.random.normal(tf.shape(image), mean=-0.5, stddev=0.1, dtype=image.dtype)\n",
    "  return image\n",
    "\n",
    "\n",
    "def make_model(input_dtype=tf.float32, transform_on_device=False):\n",
    "    \n",
    "  kernel = 3\n",
    "  pool = 2\n",
    "  dropout = 0.5\n",
    "  num_classes = 10\n",
    "\n",
    "  input_shape = get_input_shape()\n",
    "\n",
    "  input_layer = tf.keras.layers.Conv2D(32, kernel_size=(kernel, kernel),\n",
    "                                       padding='same',\n",
    "                                       activation='relu',\n",
    "                                       input_shape=input_shape,\n",
    "                                       dtype=input_dtype\n",
    "                                      )\n",
    "\n",
    "  layer = input_layer\n",
    "  if transform_on_device:\n",
    "    layer = tf.keras.layers.Lambda(transform_image)(layer)\n",
    "    \n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(layer)\n",
    "  model.add(tf.keras.layers.Conv2D(64, (kernel, kernel),  padding='same',activation='relu'))\n",
    "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(pool, pool)))\n",
    "  model.add(tf.keras.layers.Dropout(dropout))\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "  model.add(tf.keras.layers.Dense(1024, activation='relu'))        \n",
    "  model.add(tf.keras.layers.Dropout(dropout))\n",
    "  model.add(tf.keras.layers.Dense(num_classes))\n",
    "\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GlB2GqBKvWJH"
   },
   "source": [
    "## Training loop\n",
    "\n",
    "But wait, this is much more complicated than the slides...\n",
    "\n",
    "Indeed. This is because it runs all of the options discussed, and tries to clean up after itsef (Hence the context managers), profiles, and provide accurate steady state measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FIQenvZd-q93"
   },
   "source": [
    "### Toggles for various optimizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CUacwlUg-1pe"
   },
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def stop_profiler():\n",
    "  \"\"\"Used to guarantee that the TensorFlow profiler does not remain on.\n",
    "  \n",
    "  We don't want to mix traces from different runs as it would make them hard\n",
    "  to interpret, so this ensures that the profiler is disabled even if our\n",
    "  training loop crashes.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    yield\n",
    "  finally:\n",
    "    tf.summary.trace_off()\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def use_mixed_precision(loss_scale):\n",
    "  \"\"\"Enable mixed precision, and reset the policy after training.\"\"\"\n",
    "  old_policy = tf.keras.mixed_precision.experimental.global_policy()\n",
    "\n",
    "  try:\n",
    "    policy = tf.compat.v2.keras.mixed_precision.experimental.Policy(\n",
    "        \"mixed_float16\", loss_scale=loss_scale)\n",
    "    tf.keras.mixed_precision.experimental.set_policy(policy)\n",
    "    yield\n",
    "  finally:\n",
    "    tf.keras.mixed_precision.experimental.set_policy(old_policy)\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def enable_xla():\n",
    "  \"\"\"Enable XLA, and disable it after training is complete.\"\"\"\n",
    "  try:\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    yield\n",
    "  finally:\n",
    "    tf.config.optimizer.set_jit(False)\n",
    "\n",
    "\n",
    "_THREADS_PER_GPU = 2\n",
    "@contextlib.contextmanager\n",
    "def tuning_context():\n",
    "  \"\"\"Hand tuned model configurations.\n",
    "  \n",
    "  Historically these knobs have improved performance, but as of 10/28/2019 they\n",
    "  actually hurt performance. However they are provided simply for completeness\n",
    "  to show some of the lower level knobs.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    os.environ['TF_GPU_THREAD_MODE'] = \"gpu_private\"\n",
    "    os.environ['TF_GPU_THREAD_COUNT'] = str(_THREADS_PER_GPU)\n",
    "    tf.keras.backend.set_image_data_format(\"channels_first\")\n",
    "    yield\n",
    "  finally:\n",
    "    os.environ.pop('TF_GPU_THREAD_MODE', None)\n",
    "    os.environ.pop('TF_GPU_THREAD_COUNT', None)\n",
    "    tf.keras.backend.set_image_data_format(\"channels_last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tq14j9KkqG8z"
   },
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def null_context():\n",
    "  \"\"\"Implementation detail. Used if a given toggle is disabled.\"\"\"\n",
    "  yield\n",
    "\n",
    "def null_decorator(f):\n",
    "  \"\"\"Implementation detail. Used if tf.function is disabled.\"\"\"\n",
    "  return f\n",
    "\n",
    "def train_model(data_fn, global_batch_size, use_tf_function=False, \n",
    "                strategy=None, xla=False, mixed_precision=False, \n",
    "                loss_scale=\"dynamic\", collect_profile=False, tuned=False, \n",
    "                transform_on_device=False):\n",
    "\n",
    "  # Ensure runs are independent.\n",
    "  tf.keras.backend.clear_session()\n",
    "  gc.collect()\n",
    "  time.sleep(3)\n",
    "\n",
    "  if tuned:\n",
    "    assert strategy, \"Tuned version assumes a distribuation strategy is present.\"\n",
    "\n",
    "  dtype = tf.float16 if mixed_precision else tf.float32\n",
    "  step_decorator = tf.function if use_tf_function and not strategy else null_decorator\n",
    "  strategy_scope = strategy.scope() if strategy else null_context()\n",
    "  xla_scope = enable_xla() if xla else null_context()\n",
    "  precision_scope = use_mixed_precision(loss_scale) if mixed_precision else null_context()\n",
    "  tuning_scope = tuning_context() if tuned else null_context()\n",
    "\n",
    "  with strategy_scope, xla_scope, precision_scope, stop_profiler(), tuning_scope:\n",
    "    model = make_model(input_dtype=dtype, transform_on_device=transform_on_device)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "    if mixed_precision:\n",
    "      optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, loss_scale=loss_scale)\n",
    "\n",
    "#    model.compile(\n",
    "#            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#            optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "#            metrics=['accuracy'],\n",
    "#        )\n",
    "    \n",
    "    @step_decorator\n",
    "    def replica_step(features, labels):\n",
    "      with tf.GradientTape() as tape:\n",
    "        logits = model(features, training=True)\n",
    "#        replica_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels, logits)\n",
    "        replica_loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True, axis=-1)\n",
    "        loss = tf.nn.compute_average_loss(replica_loss, global_batch_size=global_batch_size)\n",
    "      grads = tape.gradient(loss, model.trainable_variables)\n",
    "      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "      return loss\n",
    "\n",
    "    step_fn = replica_step\n",
    "    if strategy and use_tf_function:\n",
    "      @tf.function\n",
    "      def replicated_step(features, labels):\n",
    "        loss = strategy.experimental_run_v2(replica_step, (features, labels))\n",
    "        return loss\n",
    "\n",
    "      step_fn = replicated_step\n",
    "\n",
    "    if strategy:\n",
    "      dataset = data_fn(batch_size=global_batch_size, dtype=dtype, \n",
    "                        transform_on_device=transform_on_device)\n",
    "      if tuned:\n",
    "            \n",
    "        options = tf.data.Options()\n",
    "        private_threads = (multiprocessing.cpu_count() - \n",
    "                           strategy.num_replicas_in_sync * (_THREADS_PER_GPU + 1))\n",
    "        options.experimental_threading.private_threadpool_size = private_threads\n",
    "        dataset = dataset.with_options(options)\n",
    "      data = strategy.experimental_distribute_dataset(dataset)\n",
    "    else:\n",
    "      assert not transform_on_device\n",
    "      data = data_fn(batch_size=global_batch_size, dtype=dtype)\n",
    "\n",
    "    schedule = [\n",
    "        5,                             # Burn in\n",
    "        5 if collect_profile else 0,   # Profiling\n",
    "        30,                            # Steady state throughput\n",
    "    ]\n",
    "    times = []\n",
    "    \n",
    "    for step_number, inputs in enumerate(data):\n",
    "      loss = step_fn(*inputs)\n",
    "\n",
    "      # Burn in\n",
    "      if schedule[0]:\n",
    "        schedule[0] -= 1\n",
    "        if not schedule[0]:\n",
    "          print(\"Burn in complete.\")\n",
    "          if schedule[1]:\n",
    "            time.sleep(2)  # Let running ops finish to start from a clean trace.\n",
    "            tf.summary.trace_on(profiler=True)\n",
    "          else:\n",
    "            # Skip straight to steady state throughput\n",
    "            start_time = timeit.default_timer()\n",
    "            iter_count = 0\n",
    "        continue\n",
    "\n",
    "      # Op profiler\n",
    "      if schedule[1]:\n",
    "        schedule[1] -= 1\n",
    "        if not schedule[1]:\n",
    "          tf.summary.trace_export(name=\"my_trace\", profiler_outdir=profile_dir)\n",
    "          start_time = timeit.default_timer()\n",
    "          iter_count = 0\n",
    "        continue\n",
    "\n",
    "      # Profile steady state execution\n",
    "      schedule[2] -= 1\n",
    "      iter_count += 1\n",
    "      times.append(timeit.default_timer())\n",
    "      if not schedule[2]:\n",
    "        break\n",
    "        \n",
    "    run_time = timeit.default_timer() - start_time\n",
    "    step_time = run_time / iter_count\n",
    "    # print(np.array(times[1:]) - np.array(times[:-1]))\n",
    "    data = {}\n",
    "    data[global_batch_size] = {\"batch_size\": global_batch_size, \"steps\": iter_count,\"Mean_step_time_sec\": step_time, \"Images_per_sec\": int(global_batch_size / step_time)}\n",
    "    json_data = json.dumps(data)\n",
    "    with open(\"stats.json\", \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "        \n",
    "    print(\"{} steps\".format(iter_count))\n",
    "    print(\"Mean step time: {:>6.2f} sec\".format(step_time))\n",
    "    print(\"Images / sec:   {:>6d}\".format(int(global_batch_size / step_time)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YAYjN5yoNF9h"
   },
   "source": [
    "## First pass at a training function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uzeWWSDYg084"
   },
   "source": [
    "### Define a generator based data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eClYVDBxPOR8"
   },
   "outputs": [],
   "source": [
    "def random_flip(image):\n",
    "  hflip = np.random.random() > 0.5\n",
    "  vflip = np.random.random() > 0.5\n",
    "  if hflip and vflip:\n",
    "    image = cv2.flip(image, -1)\n",
    "  elif hflip:\n",
    "    image = cv2.flip(image, -1)\n",
    "  elif vflip:\n",
    "    image = cv2.flip(image, 1)\n",
    "  return image\n",
    "\n",
    "def normalize_and_add_noise(image):\n",
    "  image = image.astype(np.float32) / 255 - 0.5\n",
    "  image += np.random.normal(loc=0, scale=0.1, size=image.shape)\n",
    "  return image\n",
    "\n",
    "def make_batch(features, labels):\n",
    "  x = tf.convert_to_tensor(np.stack(features, axis=0))\n",
    "  y = tf.convert_to_tensor(np.array(labels, dtype=np.float32)[:, np.newaxis])\n",
    "  features.clear()\n",
    "  labels.clear()\n",
    "  return x, y\n",
    "\n",
    "def data_generator(batch_size, **kwargs):\n",
    "  epoch_order = np.random.permutation(get_paths_and_labels())\n",
    "  features, labels = [], []\n",
    "  for image_path, label in epoch_order:\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Resize to training resolution\n",
    "    image = cv2.resize(image, RESOLUTION)\n",
    "\n",
    "    # Randomly horizontal and vertical flip\n",
    "    image = random_flip(image)\n",
    "\n",
    "    # Normalize, center, and add Gaussian noise\n",
    "    image = normalize_and_add_noise(image)\n",
    "    \n",
    "    features.append(image)\n",
    "    labels.append(label)\n",
    "    if len(features) == batch_size:\n",
    "      yield make_batch(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "cnEMGFP1egej",
    "outputId": "a651a152-1d3b-41ad-f47d-fa58740cc988"
   },
   "outputs": [],
   "source": [
    "#for batch_size in [32, 64, 128]:\n",
    "#  print(\"Batch size: {}\".format(batch_size))\n",
    "#  train_model(data_generator, batch_size)\n",
    "#  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QNqkLxasBDY0"
   },
   "source": [
    "## Add tf.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q3pznH5zSHxq"
   },
   "source": [
    "### Use the images directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jvrj6QNYSOaU"
   },
   "outputs": [],
   "source": [
    "#def make_jpg_dataset(batch_size, dtype=tf.float32, transform_on_device=False, \n",
    "#                     already_resized=False):\n",
    "#  if already_resized:\n",
    "#    raise NotImplementedError(\n",
    "#        \"`already_resized` is only implemented for the TFRecords path.\")\n",
    "#\n",
    "#  def get_bytes_and_label(filepath):\n",
    "#    image_bytes = tf.io.read_file(filepath)\n",
    "#    label = tf.strings.regex_full_match(filepath, pos_dir + \".+\")\n",
    "#    return image_bytes, tf.expand_dims(label, 0)\n",
    "#\n",
    "#  def process_image(image_bytes, label):\n",
    "#    image = tf.io.decode_jpeg(image_bytes)\n",
    "#    image = tf.cast(image, dtype)\n",
    "#    image = tf.image.resize(image, RESOLUTION)\n",
    "#\n",
    "#    if tf.shape(image)[2] == 1:\n",
    "#      # Some images are greyscale.\n",
    "#      image = tf.tile(image, (1, 1, 3))\n",
    "#\n",
    "#    image.set_shape(NHWC_INPUT_SHAPE)\n",
    "#\n",
    "#    if not transform_on_device:\n",
    "#      image = transform_image(image)\n",
    "#\n",
    "#    if tf.keras.backend.image_data_format() == \"channels_first\":\n",
    "#      image = tf.transpose(image, (2, 0, 1))\n",
    "#    \n",
    "#    return image, tf.cast(label, dtype)\n",
    "#\n",
    "#  dataset = tf.data.Dataset.list_files([pos_dir + \"/*\", neg_dir + \"/*\"])\n",
    "#  dataset = dataset.shuffle(NUM_TOTAL_IMAGES)\n",
    "#  dataset = dataset.map(get_bytes_and_label, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#  dataset = dataset.map(process_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#  dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "#  return dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A9Frb0qbSLrG"
   },
   "source": [
    "### Use TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bZQvIamLw87p"
   },
   "outputs": [],
   "source": [
    "def make_dataset(batch_size, dtype=tf.float32, transform_on_device=False, \n",
    "                 already_resized=True):\n",
    "\n",
    "  # put already_resized false if you would like to measure reading jpg files directly\n",
    "\n",
    "#  def parse_fn(record):\n",
    "#    RECORD_SCHEMA = {\n",
    "#     \"image\": tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "#     \"label\": tf.io.FixedLenFeature([1], dtype=tf.int64)\n",
    "#    }\n",
    "#    record = tf.io.parse_single_example(record, RECORD_SCHEMA)\n",
    "#    image = tf.io.decode_jpeg(record[\"image\"])\n",
    "#    image = tf.cast(image, dtype)\n",
    "#    if not already_resized:\n",
    "#      image = tf.image.resize(image, RESOLUTION)\n",
    "#\n",
    "#    if tf.shape(image)[2] == 1:\n",
    "#      # Some images are greyscale.\n",
    "#      image = tf.tile(image, (1, 1, 3))\n",
    "#\n",
    "#    image.set_shape(NHWC_INPUT_SHAPE)\n",
    "#    \n",
    "#    if not transform_on_device:\n",
    "#      image = transform_image(image)\n",
    "#\n",
    "#    if tf.keras.backend.image_data_format() == \"channels_first\":\n",
    "#      image = tf.transpose(image, (2, 0, 1))\n",
    "#    \n",
    "#    return image, tf.cast(record[\"label\"], dtype)\n",
    "\n",
    "    \n",
    "  def parse_fn(record):\n",
    "    img_rows, img_cols = 28, 28\n",
    "    RECORD_SCHEMA = {\n",
    "        'image_raw': tf.io.FixedLenFeature([img_rows * img_cols], tf.float32),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    record = tf.io.parse_single_example(record, RECORD_SCHEMA)\n",
    "    image = record['image_raw']\n",
    "    label = record['label']   \n",
    "    return image, label\n",
    "\n",
    "  def _reshape_img(image, label):\n",
    "      image = tf.reshape(image, [28, 28, 1])\n",
    "      # label = tf.one_hot(label, num_classes)\n",
    "      return image, label\n",
    "\n",
    "\n",
    "  from hops import hdfs\n",
    "  import pydoop.hdfs as pydoop\n",
    "\n",
    "#  data_dir = hdfs.project_path()\n",
    "#  filenames = pydoop.path.abspath(data_dir + \"TourData/tfrecord_data/\")\n",
    "#  pattern = tf.io.gfile.glob(filenames + \"*.tfrecords\")\n",
    "\n",
    "  data_dir = hdfs.project_path()\n",
    "  filenames = pydoop.path.abspath(data_dir + \"TourData/mnist/train/df-mnist_train.tfrecord\")\n",
    "  pattern = tf.io.gfile.glob(filenames + \"/part-r-*\")\n",
    "\n",
    "  dataset = tf.data.Dataset.list_files(pattern)\n",
    "  dataset = dataset.interleave(tf.data.TFRecordDataset, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  dataset = dataset.shuffle(4 * batch_size)\n",
    "  dataset = dataset.map(parse_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  dataset = dataset.map(\n",
    "        _reshape_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "  return dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ttFWvxziZap4"
   },
   "source": [
    "### *already_resized* and *transform_on_device*\n",
    "\n",
    "Even with maximum parallelization, the CPU can only produce a bit over 3000 examples per second. This is fine for 1 GPU training since the GPU maxes out in the low 2000's, but would prevent reasonable scaling to more GPUs. This is due to two principle bottlenecks:\n",
    "\n",
    "#### Native image size\n",
    "\n",
    "The downloaded thumbnails tend to be around 400x600 resolution, whereas we're training at 224x224. This means that we have to move approximately 6x as many bytes into memory, spend a correspondingly long time decoding the jpg's, and incur an extra memcpy for the resize. It turns out to be quite important to resize the images to 224x224 and use those resized images in the input pipeline.\n",
    "\n",
    "#### Random augmentation\n",
    "\n",
    "The CPU simply cannot add noise to the images quickly enough to keep up with the GPU, so to maintain performance for the multi-GPU case we have to move those transformations from the input pipeline to the start of the model. Even though that puts them on the critical path, the GPU can process the augmentation so quickly that it isn't an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "O8w1vYVjwQkG",
    "outputId": "89839bbf-b4e6-41b7-bf9d-536a6181cc43"
   },
   "outputs": [],
   "source": [
    "#def measure_dataset_throughput(dataset_fn, label=\"\"):\n",
    "#  count = 0\n",
    "#  bluh = 0\n",
    "#  batch_size = 32\n",
    "##  for i, _ in enumerate(dataset_fn(batch_size=batch_size).take(50)):\n",
    "#  for i, _ in enumerate(dataset_fn(batch_size=batch_size)):\n",
    "#    bluh += 1    \n",
    "#    if i == 3:\n",
    "#      st = timeit.default_timer()\n",
    "#    if i > 3:\n",
    "#      count += 1\n",
    "#  step_time = (timeit.default_timer() - st) / count\n",
    "#  print(step_time, (timeit.default_timer() - st), count, bluh, _[0].shape)  \n",
    "#  print(\"{:<45}  {:>6.0f} Images / sec\".format(label, batch_size / step_time // 100 * 100))\n",
    "#\n",
    "#\n",
    "#def make_synthetic_dataset(batch_size, dtype=tf.float32, **kwargs):\n",
    "#  num_images_per_label  = 50000\n",
    "#  dataset = tf.data.Dataset.range(2 * num_images_per_label)\n",
    "#  def map_fn(_):\n",
    "#    x = tf.zeros(shape=get_input_shape(), dtype=dtype)\n",
    "#    y = tf.zeros(shape=(1,), dtype=dtype)\n",
    "#    return x, y\n",
    "#  dataset = dataset.map(map_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#  dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "#  return dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "#\n",
    "#\n",
    "##measure_dataset_throughput(\n",
    "##    functools.partial(make_jpg_dataset, dtype=tf.float16),\n",
    "##    \"Use JPEGs directly\")\n",
    "#\n",
    "#measure_dataset_throughput(\n",
    "#    functools.partial(make_dataset, dtype=tf.float16),\n",
    "#    \"Use TFRecords\")\n",
    "#\n",
    "#measure_dataset_throughput(\n",
    "#    functools.partial(make_dataset, dtype=tf.float16, transform_on_device=True, \n",
    "#                      already_resized=True),\n",
    "#    \"Use TFRecords with scaling optimizations\")\n",
    "#\n",
    "## Use synthetic data to ensure model is not input bound.\n",
    "#measure_dataset_throughput(\n",
    "#    functools.partial(make_synthetic_dataset, dtype=tf.float16),\n",
    "#    \"Synthetic data.\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UVJnWI_OBJid"
   },
   "source": [
    "## Add tf.function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7rDLkJGIBNtU"
   },
   "source": [
    "## Add XLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WSAIDkr-BQid"
   },
   "source": [
    "## Add mixed precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1FgCboNpBYi7"
   },
   "source": [
    "## Add distribution strategy and various tuning knobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605
    },
    "colab_type": "code",
    "id": "n6YLvbXR3nB_",
    "outputId": "94032e63-ca5a-4db4-a31a-68c592096839"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 4, gpu.c.dpe-cloud-mle.internal, executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1654, in _create_c_op\n",
      "    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Negative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d/MaxPool}} = MaxPool[T=DT_HALF, data_format=\"NCHW\", ksize=[1, 1, 2, 2], padding=\"VALID\", strides=[1, 1, 2, 2]](conv2d_1/Identity)' with input shapes: [?,64,28,1].\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/hops/experiment_impl/distribute/mirrored.py\", line 149, in _wrapper_fun\n",
      "    retval = map_fun()\n",
      "  File \"<stdin>\", line 22, in mirrored_training\n",
      "  File \"<stdin>\", line 31, in train_model\n",
      "  File \"<stdin>\", line 49, in make_model\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\", line 456, in _method_wrapper\n",
      "    result = method(self, *args, **kwargs)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/keras/engine/sequential.py\", line 213, in add\n",
      "    output_tensor = layer(self.outputs[0])\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 922, in __call__\n",
      "    outputs = call_fn(cast_inputs, *args, **kwargs)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/keras/layers/pooling.py\", line 296, in call\n",
      "    data_format=conv_utils.convert_data_format(self.data_format, 4))\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 3929, in max_pool\n",
      "    name=name)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 5236, in max_pool\n",
      "    data_format=data_format, name=name)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 744, in _apply_op_helper\n",
      "    attrs=attr_protos, op_def=op_def)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 595, in _create_op_internal\n",
      "    compute_device)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3327, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1817, in __init__\n",
      "    control_input_ops, op_def)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1657, in _create_c_op\n",
      "    raise ValueError(str(e))\n",
      "ValueError: Negative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d/MaxPool}} = MaxPool[T=DT_HALF, data_format=\"NCHW\", ksize=[1, 1, 2, 2], padding=\"VALID\", strides=[1, 1, 2, 2]](conv2d_1/Identity)' with input shapes: [?,64,28,1].\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1654, in _create_c_op\n",
      "    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Negative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d/MaxPool}} = MaxPool[T=DT_HALF, data_format=\"NCHW\", ksize=[1, 1, 2, 2], padding=\"VALID\", strides=[1, 1, 2, 2]](conv2d_1/Identity)' with input shapes: [?,64,28,1].\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/hops/experiment_impl/distribute/mirrored.py\", line 149, in _wrapper_fun\n",
      "    retval = map_fun()\n",
      "  File \"<stdin>\", line 22, in mirrored_training\n",
      "  File \"<stdin>\", line 31, in train_model\n",
      "  File \"<stdin>\", line 49, in make_model\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\", line 456, in _method_wrapper\n",
      "    result = method(self, *args, **kwargs)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/keras/engine/sequential.py\", line 213, in add\n",
      "    output_tensor = layer(self.outputs[0])\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 922, in __call__\n",
      "    outputs = call_fn(cast_inputs, *args, **kwargs)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/keras/layers/pooling.py\", line 296, in call\n",
      "    data_format=conv_utils.convert_data_format(self.data_format, 4))\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 3929, in max_pool\n",
      "    name=name)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 5236, in max_pool\n",
      "    data_format=data_format, name=name)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 744, in _apply_op_helper\n",
      "    attrs=attr_protos, op_def=op_def)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 595, in _create_op_internal\n",
      "    compute_device)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3327, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1817, in __init__\n",
      "    control_input_ops, op_def)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1657, in _create_c_op\n",
      "    raise ValueError(str(e))\n",
      "ValueError: Negative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d/MaxPool}} = MaxPool[T=DT_HALF, data_format=\"NCHW\", ksize=[1, 1, 2, 2], padding=\"VALID\", strides=[1, 1, 2, 2]](conv2d_1/Identity)' with input shapes: [?,64,28,1].\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/hops/experiment.py\", line 591, in mirrored\n",
      "    logdir, return_dict = mirrored_impl._run(sc, map_fun, run_id, local_logdir=local_logdir, name=name, evaluator=evaluator)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/hops/experiment_impl/distribute/mirrored.py\", line 47, in _run\n",
      "    nodeRDD.foreachPartition(_prepare_func(app_id, run_id, map_fun, local_logdir, server_addr, evaluator, util.num_executors()))\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 806, in foreachPartition\n",
      "    self.mapPartitions(func).count()  # Force evaluation\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1055, in count\n",
      "    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1046, in sum\n",
      "    return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 917, in fold\n",
      "    vals = self.mapPartitions(func).collect()\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 816, in collect\n",
      "    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
      "  File \"/srv/hops/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/srv/hops/spark/python/lib/py4j-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 4, gpu.c.dpe-cloud-mle.internal, executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1654, in _create_c_op\n",
      "    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Negative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d/MaxPool}} = MaxPool[T=DT_HALF, data_format=\"NCHW\", ksize=[1, 1, 2, 2], padding=\"VALID\", strides=[1, 1, 2, 2]](conv2d_1/Identity)' with input shapes: [?,64,28,1].\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/hops/experiment_impl/distribute/mirrored.py\", line 149, in _wrapper_fun\n",
      "    retval = map_fun()\n",
      "  File \"<stdin>\", line 22, in mirrored_training\n",
      "  File \"<stdin>\", line 31, in train_model\n",
      "  File \"<stdin>\", line 49, in make_model\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\", line 456, in _method_wrapper\n",
      "    result = method(self, *args, **kwargs)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/keras/engine/sequential.py\", line 213, in add\n",
      "    output_tensor = layer(self.outputs[0])\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 922, in __call__\n",
      "    outputs = call_fn(cast_inputs, *args, **kwargs)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/keras/layers/pooling.py\", line 296, in call\n",
      "    data_format=conv_utils.convert_data_format(self.data_format, 4))\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 3929, in max_pool\n",
      "    name=name)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 5236, in max_pool\n",
      "    data_format=data_format, name=name)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 744, in _apply_op_helper\n",
      "    attrs=attr_protos, op_def=op_def)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 595, in _create_op_internal\n",
      "    compute_device)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3327, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1817, in __init__\n",
      "    control_input_ops, op_def)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1657, in _create_c_op\n",
      "    raise ValueError(str(e))\n",
      "ValueError: Negative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d/MaxPool}} = MaxPool[T=DT_HALF, data_format=\"NCHW\", ksize=[1, 1, 2, 2], padding=\"VALID\", strides=[1, 1, 2, 2]](conv2d_1/Identity)' with input shapes: [?,64,28,1].\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1654, in _create_c_op\n",
      "    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Negative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d/MaxPool}} = MaxPool[T=DT_HALF, data_format=\"NCHW\", ksize=[1, 1, 2, 2], padding=\"VALID\", strides=[1, 1, 2, 2]](conv2d_1/Identity)' with input shapes: [?,64,28,1].\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/hops/experiment_impl/distribute/mirrored.py\", line 149, in _wrapper_fun\n",
      "    retval = map_fun()\n",
      "  File \"<stdin>\", line 22, in mirrored_training\n",
      "  File \"<stdin>\", line 31, in train_model\n",
      "  File \"<stdin>\", line 49, in make_model\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\", line 456, in _method_wrapper\n",
      "    result = method(self, *args, **kwargs)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/keras/engine/sequential.py\", line 213, in add\n",
      "    output_tensor = layer(self.outputs[0])\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 922, in __call__\n",
      "    outputs = call_fn(cast_inputs, *args, **kwargs)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/keras/layers/pooling.py\", line 296, in call\n",
      "    data_format=conv_utils.convert_data_format(self.data_format, 4))\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 3929, in max_pool\n",
      "    name=name)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 5236, in max_pool\n",
      "    data_format=data_format, name=name)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 744, in _apply_op_helper\n",
      "    attrs=attr_protos, op_def=op_def)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 595, in _create_op_internal\n",
      "    compute_device)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3327, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1817, in __init__\n",
      "    control_input_ops, op_def)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1657, in _create_c_op\n",
      "    raise ValueError(str(e))\n",
      "ValueError: Negative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d/MaxPool}} = MaxPool[T=DT_HALF, data_format=\"NCHW\", ksize=[1, 1, 2, 2], padding=\"VALID\", strides=[1, 1, 2, 2]](conv2d_1/Identity)' with input shapes: [?,64,28,1].\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def mirrored_training():\n",
    "    \n",
    "    from hops import hdfs\n",
    "    from hops import tensorboard    \n",
    "    # How you easily get the TensorBoard logdir for summaries\n",
    "    tensorboard_logdir = tensorboard.logdir()\n",
    "    \n",
    "    for batch_size in [256]:\n",
    "      train_model(make_dataset, batch_size, strategy=tf.distribute.MirroredStrategy(), \n",
    "                  use_tf_function=True, xla=True, mixed_precision=True, collect_profile=False)\n",
    "      hdfs.copy_to_hdfs(\"stats.json\", \"Logs/tf_performance/mirrored_base-{}.json\".format(batch_size), overwrite=True)  \n",
    "\n",
    "      train_model(make_dataset, batch_size, strategy=tf.distribute.MirroredStrategy(), \n",
    "                  use_tf_function=True, xla=True, mixed_precision=True, loss_scale=128, collect_profile=False)\n",
    "      hdfs.copy_to_hdfs(\"stats.json\", \"Logs/tf_performance/mirrored_base_lossscale-{}.json\".format(batch_size), overwrite=True)  \n",
    "\n",
    "#      train_model(make_dataset, batch_size, strategy=tf.distribute.MirroredStrategy(), \n",
    "#                  use_tf_function=True, xla=True, mixed_precision=True, tuned=True, collect_profile=False)\n",
    "#      hdfs.copy_to_hdfs(\"stats.json\", \"Logs/tf_performance/mirrored_base_tuned-{}.json\".format(batch_size), overwrite=True)  \n",
    "#\n",
    "      train_model(make_dataset, batch_size, strategy=tf.distribute.MirroredStrategy(), \n",
    "                  use_tf_function=True, xla=True, mixed_precision=True, loss_scale=128, tuned=True, collect_profile=False)\n",
    "      hdfs.copy_to_hdfs(\"stats.json\", \"Logs/tf_performance/mirrored_base_lossscale_tuned-{}.json\".format(batch_size), overwrite=True)  \n",
    "\n",
    "from hops import experiment\n",
    "experiment.mirrored(mirrored_training)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zShqVRrGB1Aa"
   },
   "source": [
    "## Move to multi device\n",
    "\n",
    "In order for the input pipeline to keep up, we have to move to already resized images, and move the random augmentation onto the GPU, as the CPU cannot process the augmentation functions quickly enought even at 100% utilization. We also switch to using TFRecords instead of raw JPEGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "colab_type": "code",
    "id": "58wFWVDa-7kb",
    "outputId": "befb260c-5372-45bd-f554-7e1250886fc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 256\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Burn in complete.\n",
      "30 steps\n",
      "Mean step time:   0.11 sec\n",
      "Images / sec:     2268\n",
      "\n",
      "Batch size: 512\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "INFO:tensorflow:batch_all_reduce: 158 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce: 158 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "Burn in complete.\n",
      "30 steps\n",
      "Mean step time:   0.12 sec\n",
      "Images / sec:     4269\n",
      "\n",
      "Batch size: 1024\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "INFO:tensorflow:batch_all_reduce: 158 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce: 158 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "Burn in complete.\n",
      "30 steps\n",
      "Mean step time:   0.13 sec\n",
      "Images / sec:     7874\n",
      "\n",
      "Batch size: 2048\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7')\n",
      "INFO:tensorflow:batch_all_reduce: 158 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce: 158 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "Burn in complete.\n",
      "30 steps\n",
      "Mean step time:   0.14 sec\n",
      "Images / sec:    14285\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num_gpus in [1, 2, 4, 8]:\n",
    "  for batch_size in [256 * num_gpus]:\n",
    "    print(\"Batch size: {}\".format(batch_size))\n",
    "    \n",
    "    train_model(functools.partial(make_dataset, already_resized=True), \n",
    "                batch_size, \n",
    "                strategy=tf.distribute.MirroredStrategy([\"GPU:{}\".format(i) for i in range(num_gpus)]), \n",
    "                use_tf_function=True, \n",
    "                xla=True, \n",
    "                mixed_precision=True,\n",
    "                transform_on_device=True,\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S3k44OU66QXM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tf_world.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}