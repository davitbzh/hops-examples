{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xIY9vKnUU82o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset mnist/3.0.0 (download: 11.06 MiB, generated: Unknown size, total: 11.06 MiB) to hdfs://10.128.0.3:8020/Projects/demo_deep_learning_admin000/TourData/mnist/3.0.0...\u001b[0m\n",
      "\n",
      "\u001b[1mDataset mnist downloaded and prepared to hdfs://10.128.0.3:8020/Projects/demo_deep_learning_admin000/TourData/mnist/3.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
      "Dl Completed...: 100%|##########| 4/4 [00:04<00:00,  1.15s/ file]"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import pydoop.hdfs as pydoop\n",
    "from hops import hdfs\n",
    "    \n",
    "data_dir = hdfs.project_path()\n",
    "tf_datasets = pydoop.path.abspath(data_dir + \"TourData/\")\n",
    "\n",
    "datasets, info = tfds.load(name='mnist',\n",
    "                                data_dir=tf_datasets, \n",
    "                                with_info=True,\n",
    "                                as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "from hops import hdfs\n",
    "import pydoop.hdfs as pydoop\n",
    "\n",
    "dataset_dir = hdfs.project_path()\n",
    "data_dir = pydoop.path.abspath(dataset_dir + \"TourData/mnist/3.0.0\")\n",
    "\n",
    "datasets, info = tfds.load(name='mnist',\n",
    "                                with_info=True,\n",
    "                                as_supervised=True,\n",
    "                                download=True,\n",
    "                                data_dir = data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "#for elem in datasets['test']:\n",
    "#  print(elem)\n",
    "\n",
    "datasets_train = datasets['train'].batch(10)\n",
    "\n",
    "it = iter(datasets_train)\n",
    "print(next(it)[1].numpy().shape)\n",
    "\n",
    "print(tf.convert_to_tensor([0, 1, 2]).numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = hdfs.project_path()\n",
    "train_filenames = pydoop.path.abspath(data_dir + \"TourData/mnist/train/df-mnist_train.tfrecord\")\n",
    "train_filenames = tf.io.gfile.glob(train_filenames + \"/part-r-*\")\n",
    "validation_filenames = pydoop.path.abspath(data_dir + \"TourData/mnist/validation/df-mnist_test.tfrecord\")\n",
    "validation_filenames = tf.io.gfile.glob(validation_filenames + \"/part-r-*\")\n",
    "\n",
    "BATCH_SIZE = 32 \n",
    "SHUFFLE_SIZE = BATCH_SIZE * 4\n",
    "\n",
    "num_classes = 10\n",
    "epochs = 3\n",
    "kernel = 3\n",
    "pool = 2\n",
    "dropout = 0.5    \n",
    "\n",
    "# Input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "\n",
    "def input_fn(filenames, BATCH_SIZE):\n",
    "      \n",
    "    \n",
    "      def _parser(serialized_example):\n",
    "            \"\"\"Parses a single tf.Example into image and label tensors.\"\"\"\n",
    "            features = tf.io.parse_single_example(\n",
    "                serialized_example,\n",
    "                features={\n",
    "                    'image_raw': tf.io.FixedLenFeature([img_rows * img_cols], tf.float32),                    \n",
    "                    'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "                })\n",
    "            \n",
    "            image = features['image_raw']\n",
    "            label = features['label']   \n",
    "                \n",
    "            return image, label\n",
    "      \n",
    "    \n",
    "      def _normalize_img(image, label):\n",
    "            \"\"\"Normalizes images\"\"\"\n",
    "            image = tf.cast(image, tf.float32) / 255 #* (1. / 255) - 0.5\n",
    "            label = tf.cast(label, tf.int32)        \n",
    "            return image, label\n",
    "\n",
    "      def _reshape_img(image, label):\n",
    "        image = tf.reshape(image, [28, 28, 1])\n",
    "        label = label #tf.one_hot(label, num_classes)\n",
    "        return image, label\n",
    "        \n",
    "        \n",
    "      # Import MNIST data\n",
    "      dataset = tf.data.TFRecordDataset(filenames)\n",
    "        \n",
    "      # Map the parser over dataset, and batch results by up to batch_size\n",
    "      dataset = dataset.map(_parser, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "        \n",
    "      dataset = dataset.map(\n",
    "        _normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "      dataset = dataset.map(\n",
    "        _reshape_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "      dataset = dataset.repeat(500*100)\n",
    "      dataset = dataset.cache()\n",
    "      dataset = dataset.shuffle(SHUFFLE_SIZE)\n",
    "      dataset = dataset.batch(BATCH_SIZE)\n",
    "      return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "val_data = input_fn(validation_filenames, BATCH_SIZE)    \n",
    "it = iter(val_data)\n",
    "print(next(it)[1].numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper():\n",
    "    import tensorflow_datasets as tfds\n",
    "    import tensorflow as tf\n",
    "    from hops import tensorboard\n",
    "    from hops import hdfs\n",
    "    import pydoop.hdfs as pydoop\n",
    "    \n",
    "    #strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    \n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 64\n",
    "    \n",
    "    import os\n",
    "    \n",
    "    dataset_dir = hdfs.project_path()\n",
    "    data_dir = pydoop.path.abspath(dataset_dir + \"TourData/mnist/3.0.0\")\n",
    "\n",
    "    def make_datasets_unbatched():\n",
    "      # Scaling MNIST data from (0, 255] to (0., 1.]\n",
    "      def scale(image, label):\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image /= 255\n",
    "        return image, label\n",
    "\n",
    "      datasets, info = tfds.load(name='mnist',\n",
    "                                with_info=True,\n",
    "                                as_supervised=True,\n",
    "                                download=True,\n",
    "                                data_dir = data_dir)\n",
    "\n",
    "      return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)\n",
    "\n",
    "    train_datasets = make_datasets_unbatched().batch(BATCH_SIZE)\n",
    "    \n",
    "    print (\"#############################\")\n",
    "    print (train_datasets)\n",
    "    print (\"#############################\")\n",
    "    \n",
    "    def build_and_compile_cnn_model():\n",
    "      model = tf.keras.Sequential([\n",
    "          tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "          tf.keras.layers.MaxPooling2D(),\n",
    "          tf.keras.layers.Flatten(),\n",
    "          tf.keras.layers.Dense(64, activation='relu'),\n",
    "          tf.keras.layers.Dense(10)\n",
    "      ])\n",
    "      model.compile(\n",
    "          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "          optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
    "          metrics=['accuracy'])\n",
    "      return model\n",
    "    \n",
    "    NUM_WORKERS = 3\n",
    "    # Here the batch size scales up by number of workers since \n",
    "    # `tf.data.Dataset.batch` expects the global batch size. Previously we used 64, \n",
    "    # and now this becomes 128.\n",
    "    GLOBAL_BATCH_SIZE = 64 * NUM_WORKERS\n",
    "\n",
    "    # Creation of dataset needs to be after MultiWorkerMirroredStrategy object\n",
    "    # is instantiated.\n",
    "    train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)\n",
    "    with strategy.scope():\n",
    "      # Model building/compiling need to be within `strategy.scope()`.\n",
    "      multi_worker_model = build_and_compile_cnn_model()\n",
    "\n",
    "    # Keras' `model.fit()` trains the model with specified number of epochs and\n",
    "    # number of steps per epoch. Note that the numbers here are for demonstration\n",
    "    # purposes only and may not sufficiently produce a model with good quality.\n",
    "    tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tensorboard.logdir())\n",
    "    model_callback = tf.keras.callbacks.ModelCheckpoint(tensorboard.logdir())\n",
    "\n",
    "    multi_worker_model.fit(x=train_datasets, epochs=150, steps_per_epoch=50, callbacks=[tb_callback, model_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment \n",
      "\n",
      "('hdfs://10.128.0.3:8020/Projects/demo_deep_learning_admin000/Experiments/application_1586507110993_0012_5', {'log': 'Experiments/application_1586507110993_0012_5/chief_0_output.log'})"
     ]
    }
   ],
   "source": [
    "from hops import experiment\n",
    "#experiment.mirrored(wrapper, evaluator=True)\n",
    "experiment.mirrored(wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "multi_worker_with_keras.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "python-demo_deep_learning_admin000__meb10000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}